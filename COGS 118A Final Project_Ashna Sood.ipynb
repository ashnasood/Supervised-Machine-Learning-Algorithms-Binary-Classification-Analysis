{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ashna Sood COGS 118A Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although I did not end up keeping the Adult dataset as one of my final datasets (due to the mulitple days it took for the algorithms to run with that dataset), I am still displaying the data cleaning I perfomed to get the dataset ready for potential analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in adult dataset\n",
    "adult_df = pd.read_csv(\"data/adult.data\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column names\n",
    "adult_df.columns = [\"Age\", \"Workclass\", \"Final Weight\", \"Education\", \"Education Num\", \n",
    "                    \"Marital Status\", \"Occupation\",\"Relationship\", \"Race\", \"Sex\", \n",
    "                    \"Capital Gain\", \"Capital Loss\", \"Hours Per Week\",\n",
    "                    \"Native Country\", \"Income\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize the income col to 1 for >50K and 0 for <=50K\n",
    "adult_df[\"Income\"] = adult_df[\"Income\"].apply(lambda x: 1 if x == \" >50K\" else 0)\n",
    "\n",
    "# binarize the sex col to 1 - Female and 0 - Male\n",
    "adult_df[\"Sex\"] = adult_df[\"Sex\"].apply(lambda x: 1 if x == \" Female\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income count:\n",
      " 0    24720\n",
      "1     7841\n",
      "Name: Income, dtype: int64\n",
      "Sex count:\n",
      " 0    21790\n",
      "1    10771\n",
      "Name: Sex, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Print the counts of the newly binarized columns\n",
    "print(\"Income count:\\n\", adult_df['Income'].value_counts())\n",
    "\n",
    "print(\"Sex count:\\n\", adult_df['Sex'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Workclass</th>\n",
       "      <th>Final Weight</th>\n",
       "      <th>Education</th>\n",
       "      <th>Education Num</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Capital Gain</th>\n",
       "      <th>Capital Loss</th>\n",
       "      <th>Hours Per Week</th>\n",
       "      <th>Native Country</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age          Workclass  Final Weight    Education  Education Num  \\\n",
       "0       39          State-gov         77516    Bachelors             13   \n",
       "1       50   Self-emp-not-inc         83311    Bachelors             13   \n",
       "2       38            Private        215646      HS-grad              9   \n",
       "3       53            Private        234721         11th              7   \n",
       "4       28            Private        338409    Bachelors             13   \n",
       "...    ...                ...           ...          ...            ...   \n",
       "32556   27            Private        257302   Assoc-acdm             12   \n",
       "32557   40            Private        154374      HS-grad              9   \n",
       "32558   58            Private        151910      HS-grad              9   \n",
       "32559   22            Private        201490      HS-grad              9   \n",
       "32560   52       Self-emp-inc        287927      HS-grad              9   \n",
       "\n",
       "            Marital Status          Occupation    Relationship    Race  Sex  \\\n",
       "0            Never-married        Adm-clerical   Not-in-family   White    0   \n",
       "1       Married-civ-spouse     Exec-managerial         Husband   White    0   \n",
       "2                 Divorced   Handlers-cleaners   Not-in-family   White    0   \n",
       "3       Married-civ-spouse   Handlers-cleaners         Husband   Black    0   \n",
       "4       Married-civ-spouse      Prof-specialty            Wife   Black    1   \n",
       "...                    ...                 ...             ...     ...  ...   \n",
       "32556   Married-civ-spouse        Tech-support            Wife   White    1   \n",
       "32557   Married-civ-spouse   Machine-op-inspct         Husband   White    0   \n",
       "32558              Widowed        Adm-clerical       Unmarried   White    1   \n",
       "32559        Never-married        Adm-clerical       Own-child   White    0   \n",
       "32560   Married-civ-spouse     Exec-managerial            Wife   White    1   \n",
       "\n",
       "       Capital Gain  Capital Loss  Hours Per Week  Native Country  Income  \n",
       "0              2174             0              40   United-States       0  \n",
       "1                 0             0              13   United-States       0  \n",
       "2                 0             0              40   United-States       0  \n",
       "3                 0             0              40   United-States       0  \n",
       "4                 0             0              40            Cuba       0  \n",
       "...             ...           ...             ...             ...     ...  \n",
       "32556             0             0              38   United-States       0  \n",
       "32557             0             0              40   United-States       1  \n",
       "32558             0             0              40   United-States       0  \n",
       "32559             0             0              20   United-States       0  \n",
       "32560         15024             0              40   United-States       1  \n",
       "\n",
       "[32561 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Workclass</th>\n",
       "      <th>Final Weight</th>\n",
       "      <th>Education</th>\n",
       "      <th>Education Num</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Race</th>\n",
       "      <th>Sex</th>\n",
       "      <th>...</th>\n",
       "      <th>NativeC_ Portugal</th>\n",
       "      <th>NativeC_ Puerto-Rico</th>\n",
       "      <th>NativeC_ Scotland</th>\n",
       "      <th>NativeC_ South</th>\n",
       "      <th>NativeC_ Taiwan</th>\n",
       "      <th>NativeC_ Thailand</th>\n",
       "      <th>NativeC_ Trinadad&amp;Tobago</th>\n",
       "      <th>NativeC_ United-States</th>\n",
       "      <th>NativeC_ Vietnam</th>\n",
       "      <th>NativeC_ Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age          Workclass  Final Weight    Education  Education Num  \\\n",
       "0       39          State-gov         77516    Bachelors             13   \n",
       "1       50   Self-emp-not-inc         83311    Bachelors             13   \n",
       "2       38            Private        215646      HS-grad              9   \n",
       "3       53            Private        234721         11th              7   \n",
       "4       28            Private        338409    Bachelors             13   \n",
       "...    ...                ...           ...          ...            ...   \n",
       "32556   27            Private        257302   Assoc-acdm             12   \n",
       "32557   40            Private        154374      HS-grad              9   \n",
       "32558   58            Private        151910      HS-grad              9   \n",
       "32559   22            Private        201490      HS-grad              9   \n",
       "32560   52       Self-emp-inc        287927      HS-grad              9   \n",
       "\n",
       "            Marital Status          Occupation    Relationship    Race  Sex  \\\n",
       "0            Never-married        Adm-clerical   Not-in-family   White    0   \n",
       "1       Married-civ-spouse     Exec-managerial         Husband   White    0   \n",
       "2                 Divorced   Handlers-cleaners   Not-in-family   White    0   \n",
       "3       Married-civ-spouse   Handlers-cleaners         Husband   Black    0   \n",
       "4       Married-civ-spouse      Prof-specialty            Wife   Black    1   \n",
       "...                    ...                 ...             ...     ...  ...   \n",
       "32556   Married-civ-spouse        Tech-support            Wife   White    1   \n",
       "32557   Married-civ-spouse   Machine-op-inspct         Husband   White    0   \n",
       "32558              Widowed        Adm-clerical       Unmarried   White    1   \n",
       "32559        Never-married        Adm-clerical       Own-child   White    0   \n",
       "32560   Married-civ-spouse     Exec-managerial            Wife   White    1   \n",
       "\n",
       "       ...  NativeC_ Portugal  NativeC_ Puerto-Rico  NativeC_ Scotland  \\\n",
       "0      ...                  0                     0                  0   \n",
       "1      ...                  0                     0                  0   \n",
       "2      ...                  0                     0                  0   \n",
       "3      ...                  0                     0                  0   \n",
       "4      ...                  0                     0                  0   \n",
       "...    ...                ...                   ...                ...   \n",
       "32556  ...                  0                     0                  0   \n",
       "32557  ...                  0                     0                  0   \n",
       "32558  ...                  0                     0                  0   \n",
       "32559  ...                  0                     0                  0   \n",
       "32560  ...                  0                     0                  0   \n",
       "\n",
       "      NativeC_ South  NativeC_ Taiwan  NativeC_ Thailand  \\\n",
       "0                  0                0                  0   \n",
       "1                  0                0                  0   \n",
       "2                  0                0                  0   \n",
       "3                  0                0                  0   \n",
       "4                  0                0                  0   \n",
       "...              ...              ...                ...   \n",
       "32556              0                0                  0   \n",
       "32557              0                0                  0   \n",
       "32558              0                0                  0   \n",
       "32559              0                0                  0   \n",
       "32560              0                0                  0   \n",
       "\n",
       "       NativeC_ Trinadad&Tobago  NativeC_ United-States  NativeC_ Vietnam  \\\n",
       "0                             0                       1                 0   \n",
       "1                             0                       1                 0   \n",
       "2                             0                       1                 0   \n",
       "3                             0                       1                 0   \n",
       "4                             0                       0                 0   \n",
       "...                         ...                     ...               ...   \n",
       "32556                         0                       1                 0   \n",
       "32557                         0                       1                 0   \n",
       "32558                         0                       1                 0   \n",
       "32559                         0                       1                 0   \n",
       "32560                         0                       1                 0   \n",
       "\n",
       "       NativeC_ Yugoslavia  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        0  \n",
       "4                        0  \n",
       "...                    ...  \n",
       "32556                    0  \n",
       "32557                    0  \n",
       "32558                    0  \n",
       "32559                    0  \n",
       "32560                    0  \n",
       "\n",
       "[32561 rows x 115 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding for nominal data: Workclass, Education, Marital Status, \n",
    "# Occupation, Relationship, Race, Native Country\n",
    "workclass_dummies = pd.get_dummies(adult_df[\"Workclass\"], prefix='Work')\n",
    "adult_df = pd.concat([adult_df, workclass_dummies], axis=1)\n",
    "\n",
    "edu_dummies = pd.get_dummies(adult_df[\"Education\"], prefix='Edu')\n",
    "adult_df = pd.concat([adult_df, edu_dummies], axis=1)\n",
    "\n",
    "married_dummies = pd.get_dummies(adult_df[\"Marital Status\"], prefix='Mar')\n",
    "adult_df = pd.concat([adult_df, married_dummies], axis=1)\n",
    "\n",
    "occ_dummies = pd.get_dummies(adult_df[\"Occupation\"], prefix='Occ')\n",
    "adult_df = pd.concat([adult_df, occ_dummies], axis=1)\n",
    "\n",
    "rel_dummies = pd.get_dummies(adult_df[\"Relationship\"], prefix='Rel')\n",
    "adult_df = pd.concat([adult_df, rel_dummies], axis=1)\n",
    "\n",
    "race_dummies = pd.get_dummies(adult_df[\"Race\"], prefix='Race')\n",
    "adult_df = pd.concat([adult_df, race_dummies], axis=1)\n",
    "\n",
    "nativeC_dummies = pd.get_dummies(adult_df[\"Native Country\"], prefix='NativeC')\n",
    "adult_df = pd.concat([adult_df, nativeC_dummies], axis=1)\n",
    "\n",
    "adult_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Final Weight</th>\n",
       "      <th>Education Num</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Capital Gain</th>\n",
       "      <th>Capital Loss</th>\n",
       "      <th>Hours Per Week</th>\n",
       "      <th>Income</th>\n",
       "      <th>Work_ ?</th>\n",
       "      <th>Work_ Federal-gov</th>\n",
       "      <th>...</th>\n",
       "      <th>NativeC_ Portugal</th>\n",
       "      <th>NativeC_ Puerto-Rico</th>\n",
       "      <th>NativeC_ Scotland</th>\n",
       "      <th>NativeC_ South</th>\n",
       "      <th>NativeC_ Taiwan</th>\n",
       "      <th>NativeC_ Thailand</th>\n",
       "      <th>NativeC_ Trinadad&amp;Tobago</th>\n",
       "      <th>NativeC_ United-States</th>\n",
       "      <th>NativeC_ Vietnam</th>\n",
       "      <th>NativeC_ Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>257302</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>154374</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>151910</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>201490</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>287927</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  Final Weight  Education Num  Sex  Capital Gain  Capital Loss  \\\n",
       "0       39         77516             13    0          2174             0   \n",
       "1       50         83311             13    0             0             0   \n",
       "2       38        215646              9    0             0             0   \n",
       "3       53        234721              7    0             0             0   \n",
       "4       28        338409             13    1             0             0   \n",
       "...    ...           ...            ...  ...           ...           ...   \n",
       "32556   27        257302             12    1             0             0   \n",
       "32557   40        154374              9    0             0             0   \n",
       "32558   58        151910              9    1             0             0   \n",
       "32559   22        201490              9    0             0             0   \n",
       "32560   52        287927              9    1         15024             0   \n",
       "\n",
       "       Hours Per Week  Income  Work_ ?  Work_ Federal-gov  ...  \\\n",
       "0                  40       0        0                  0  ...   \n",
       "1                  13       0        0                  0  ...   \n",
       "2                  40       0        0                  0  ...   \n",
       "3                  40       0        0                  0  ...   \n",
       "4                  40       0        0                  0  ...   \n",
       "...               ...     ...      ...                ...  ...   \n",
       "32556              38       0        0                  0  ...   \n",
       "32557              40       1        0                  0  ...   \n",
       "32558              40       0        0                  0  ...   \n",
       "32559              20       0        0                  0  ...   \n",
       "32560              40       1        0                  0  ...   \n",
       "\n",
       "       NativeC_ Portugal  NativeC_ Puerto-Rico  NativeC_ Scotland  \\\n",
       "0                      0                     0                  0   \n",
       "1                      0                     0                  0   \n",
       "2                      0                     0                  0   \n",
       "3                      0                     0                  0   \n",
       "4                      0                     0                  0   \n",
       "...                  ...                   ...                ...   \n",
       "32556                  0                     0                  0   \n",
       "32557                  0                     0                  0   \n",
       "32558                  0                     0                  0   \n",
       "32559                  0                     0                  0   \n",
       "32560                  0                     0                  0   \n",
       "\n",
       "       NativeC_ South  NativeC_ Taiwan  NativeC_ Thailand  \\\n",
       "0                   0                0                  0   \n",
       "1                   0                0                  0   \n",
       "2                   0                0                  0   \n",
       "3                   0                0                  0   \n",
       "4                   0                0                  0   \n",
       "...               ...              ...                ...   \n",
       "32556               0                0                  0   \n",
       "32557               0                0                  0   \n",
       "32558               0                0                  0   \n",
       "32559               0                0                  0   \n",
       "32560               0                0                  0   \n",
       "\n",
       "       NativeC_ Trinadad&Tobago  NativeC_ United-States  NativeC_ Vietnam  \\\n",
       "0                             0                       1                 0   \n",
       "1                             0                       1                 0   \n",
       "2                             0                       1                 0   \n",
       "3                             0                       1                 0   \n",
       "4                             0                       0                 0   \n",
       "...                         ...                     ...               ...   \n",
       "32556                         0                       1                 0   \n",
       "32557                         0                       1                 0   \n",
       "32558                         0                       1                 0   \n",
       "32559                         0                       1                 0   \n",
       "32560                         0                       1                 0   \n",
       "\n",
       "       NativeC_ Yugoslavia  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        0  \n",
       "4                        0  \n",
       "...                    ...  \n",
       "32556                    0  \n",
       "32557                    0  \n",
       "32558                    0  \n",
       "32559                    0  \n",
       "32560                    0  \n",
       "\n",
       "[32561 rows x 108 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the nominal columns that were one hot encoded\n",
    "adult_df = adult_df.drop(columns=[\"Workclass\", \"Education\", \"Marital Status\", \"Occupation\", \n",
    "                                  \"Relationship\", \"Race\", \"Native Country\"])\n",
    "adult_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Final Weight</th>\n",
       "      <th>Education Num</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Capital Gain</th>\n",
       "      <th>Capital Loss</th>\n",
       "      <th>Hours Per Week</th>\n",
       "      <th>Work_ ?</th>\n",
       "      <th>Work_ Federal-gov</th>\n",
       "      <th>Work_ Local-gov</th>\n",
       "      <th>...</th>\n",
       "      <th>NativeC_ Puerto-Rico</th>\n",
       "      <th>NativeC_ Scotland</th>\n",
       "      <th>NativeC_ South</th>\n",
       "      <th>NativeC_ Taiwan</th>\n",
       "      <th>NativeC_ Thailand</th>\n",
       "      <th>NativeC_ Trinadad&amp;Tobago</th>\n",
       "      <th>NativeC_ United-States</th>\n",
       "      <th>NativeC_ Vietnam</th>\n",
       "      <th>NativeC_ Yugoslavia</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>257302</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>154374</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>151910</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>201490</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>287927</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  Final Weight  Education Num  Sex  Capital Gain  Capital Loss  \\\n",
       "0       39         77516             13    0          2174             0   \n",
       "1       50         83311             13    0             0             0   \n",
       "2       38        215646              9    0             0             0   \n",
       "3       53        234721              7    0             0             0   \n",
       "4       28        338409             13    1             0             0   \n",
       "...    ...           ...            ...  ...           ...           ...   \n",
       "32556   27        257302             12    1             0             0   \n",
       "32557   40        154374              9    0             0             0   \n",
       "32558   58        151910              9    1             0             0   \n",
       "32559   22        201490              9    0             0             0   \n",
       "32560   52        287927              9    1         15024             0   \n",
       "\n",
       "       Hours Per Week  Work_ ?  Work_ Federal-gov  Work_ Local-gov  ...  \\\n",
       "0                  40        0                  0                0  ...   \n",
       "1                  13        0                  0                0  ...   \n",
       "2                  40        0                  0                0  ...   \n",
       "3                  40        0                  0                0  ...   \n",
       "4                  40        0                  0                0  ...   \n",
       "...               ...      ...                ...              ...  ...   \n",
       "32556              38        0                  0                0  ...   \n",
       "32557              40        0                  0                0  ...   \n",
       "32558              40        0                  0                0  ...   \n",
       "32559              20        0                  0                0  ...   \n",
       "32560              40        0                  0                0  ...   \n",
       "\n",
       "       NativeC_ Puerto-Rico  NativeC_ Scotland  NativeC_ South  \\\n",
       "0                         0                  0               0   \n",
       "1                         0                  0               0   \n",
       "2                         0                  0               0   \n",
       "3                         0                  0               0   \n",
       "4                         0                  0               0   \n",
       "...                     ...                ...             ...   \n",
       "32556                     0                  0               0   \n",
       "32557                     0                  0               0   \n",
       "32558                     0                  0               0   \n",
       "32559                     0                  0               0   \n",
       "32560                     0                  0               0   \n",
       "\n",
       "       NativeC_ Taiwan  NativeC_ Thailand  NativeC_ Trinadad&Tobago  \\\n",
       "0                    0                  0                         0   \n",
       "1                    0                  0                         0   \n",
       "2                    0                  0                         0   \n",
       "3                    0                  0                         0   \n",
       "4                    0                  0                         0   \n",
       "...                ...                ...                       ...   \n",
       "32556                0                  0                         0   \n",
       "32557                0                  0                         0   \n",
       "32558                0                  0                         0   \n",
       "32559                0                  0                         0   \n",
       "32560                0                  0                         0   \n",
       "\n",
       "       NativeC_ United-States  NativeC_ Vietnam  NativeC_ Yugoslavia  Income  \n",
       "0                           1                 0                    0       0  \n",
       "1                           1                 0                    0       0  \n",
       "2                           1                 0                    0       0  \n",
       "3                           1                 0                    0       0  \n",
       "4                           0                 0                    0       0  \n",
       "...                       ...               ...                  ...     ...  \n",
       "32556                       1                 0                    0       0  \n",
       "32557                       1                 0                    0       1  \n",
       "32558                       1                 0                    0       0  \n",
       "32559                       1                 0                    0       0  \n",
       "32560                       1                 0                    0       1  \n",
       "\n",
       "[32561 rows x 108 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move income column to the end of the df as it is the Y (what to predict) \n",
    "columns = list(adult_df.columns.values)\n",
    "# remove Income from the list and add back to end of df\n",
    "columns.pop(columns.index(\"Income\"))\n",
    "adult_df = adult_df[columns + [\"Income\"]]\n",
    "adult_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in letter dataset\n",
    "letter_df = pd.read_csv(\"data/letter-recognition.data\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column names\n",
    "letter_df.columns = [\"Letter\", \"X-box\", \"Y-box\", \"Width\", \"Height\", \"Total Pixels\", \n",
    "                    \"X-bar\",\"Y-bar\", \"X2bar\", \"Y2bar\", \"Xybar\", \"X2ybr\", \"Xy2br\",\n",
    "                    \"X-edge\", \"X-edgey\", \"Y-edge\", \"Y-edgex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Letter</th>\n",
       "      <th>X-box</th>\n",
       "      <th>Y-box</th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Total Pixels</th>\n",
       "      <th>X-bar</th>\n",
       "      <th>Y-bar</th>\n",
       "      <th>X2bar</th>\n",
       "      <th>Y2bar</th>\n",
       "      <th>Xybar</th>\n",
       "      <th>X2ybr</th>\n",
       "      <th>Xy2br</th>\n",
       "      <th>X-edge</th>\n",
       "      <th>X-edgey</th>\n",
       "      <th>Y-edge</th>\n",
       "      <th>Y-edgex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>D</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>C</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>T</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Letter  X-box  Y-box  Width  Height  Total Pixels  X-bar  Y-bar  X2bar  \\\n",
       "0          T      2      8      3       5             1      8     13      0   \n",
       "1          I      5     12      3       7             2     10      5      5   \n",
       "2          D      4     11      6       8             6     10      6      2   \n",
       "3          N      7     11      6       6             3      5      9      4   \n",
       "4          G      2      1      3       1             1      8      6      6   \n",
       "...      ...    ...    ...    ...     ...           ...    ...    ...    ...   \n",
       "19995      D      2      2      3       3             2      7      7      7   \n",
       "19996      C      7     10      8       8             4      4      8      6   \n",
       "19997      T      6      9      6       7             5      6     11      3   \n",
       "19998      S      2      3      4       2             1      8      7      2   \n",
       "19999      A      4      9      6       6             2      9      5      3   \n",
       "\n",
       "       Y2bar  Xybar  X2ybr  Xy2br  X-edge  X-edgey  Y-edge  Y-edgex  \n",
       "0          6      6     10      8       0        8       0        8  \n",
       "1          4     13      3      9       2        8       4       10  \n",
       "2          6     10      3      7       3        7       3        9  \n",
       "3          6      4      4     10       6       10       2        8  \n",
       "4          6      6      5      9       1        7       5       10  \n",
       "...      ...    ...    ...    ...     ...      ...     ...      ...  \n",
       "19995      6      6      6      4       2        8       3        7  \n",
       "19996      9     12      9     13       2        9       3        7  \n",
       "19997      7     11      9      5       2       12       2        4  \n",
       "19998      6     10      6      8       1        9       5        8  \n",
       "19999      1      8      1      8       2        7       2        8  \n",
       "\n",
       "[20000 rows x 17 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10060\n",
       "1     9940\n",
       "Name: Letter, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binarize the Letter col to make letters A-M as positive - 1 and N-Z as negative - 0\n",
    "A_M = list(map(chr, range(65, 78)))\n",
    "\n",
    "letter_df[\"Letter\"] = letter_df[\"Letter\"].apply(lambda x: 1 if x in A_M else 0)\n",
    "# check the counts of the newly binarized column\n",
    "letter_df[\"Letter\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Letter</th>\n",
       "      <th>X-box</th>\n",
       "      <th>Y-box</th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Total Pixels</th>\n",
       "      <th>X-bar</th>\n",
       "      <th>Y-bar</th>\n",
       "      <th>X2bar</th>\n",
       "      <th>Y2bar</th>\n",
       "      <th>Xybar</th>\n",
       "      <th>X2ybr</th>\n",
       "      <th>Xy2br</th>\n",
       "      <th>X-edge</th>\n",
       "      <th>X-edgey</th>\n",
       "      <th>Y-edge</th>\n",
       "      <th>Y-edgex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Letter  X-box  Y-box  Width  Height  Total Pixels  X-bar  Y-bar  X2bar  \\\n",
       "0           0      2      8      3       5             1      8     13      0   \n",
       "1           1      5     12      3       7             2     10      5      5   \n",
       "2           1      4     11      6       8             6     10      6      2   \n",
       "3           0      7     11      6       6             3      5      9      4   \n",
       "4           1      2      1      3       1             1      8      6      6   \n",
       "...       ...    ...    ...    ...     ...           ...    ...    ...    ...   \n",
       "19995       1      2      2      3       3             2      7      7      7   \n",
       "19996       1      7     10      8       8             4      4      8      6   \n",
       "19997       0      6      9      6       7             5      6     11      3   \n",
       "19998       0      2      3      4       2             1      8      7      2   \n",
       "19999       1      4      9      6       6             2      9      5      3   \n",
       "\n",
       "       Y2bar  Xybar  X2ybr  Xy2br  X-edge  X-edgey  Y-edge  Y-edgex  \n",
       "0          6      6     10      8       0        8       0        8  \n",
       "1          4     13      3      9       2        8       4       10  \n",
       "2          6     10      3      7       3        7       3        9  \n",
       "3          6      4      4     10       6       10       2        8  \n",
       "4          6      6      5      9       1        7       5       10  \n",
       "...      ...    ...    ...    ...     ...      ...     ...      ...  \n",
       "19995      6      6      6      4       2        8       3        7  \n",
       "19996      9     12      9     13       2        9       3        7  \n",
       "19997      7     11      9      5       2       12       2        4  \n",
       "19998      6     10      6      8       1        9       5        8  \n",
       "19999      1      8      1      8       2        7       2        8  \n",
       "\n",
       "[20000 rows x 17 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covertype Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in Covertype dataset\n",
    "covertype_df = pd.read_csv(\"data/covtype.data\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add / rename column names\n",
    "covertype_df = covertype_df.rename(columns = {0: \"Elevation\", 1: \"Aspect\", 2: \"Slope\", \n",
    "                    3: \"Horizontal_Distance_To_Hydrology\", 4: \"Vertical_Distance_To_Hydrology\", \n",
    "                    5: \"Horizontal_Distance_To_Roadways\", 6: \"Hillshade_9am\", 7: \"Hillshade_Noon\", \n",
    "                    8: \"Hillshade_3pm\", 9: \"Horizontal_Distance_To_Fire_Points\", 54: \"Cover Type\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>Cover Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581007</th>\n",
       "      <td>2396</td>\n",
       "      <td>153</td>\n",
       "      <td>20</td>\n",
       "      <td>85</td>\n",
       "      <td>17</td>\n",
       "      <td>108</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>118</td>\n",
       "      <td>837</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581008</th>\n",
       "      <td>2391</td>\n",
       "      <td>152</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>119</td>\n",
       "      <td>845</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581009</th>\n",
       "      <td>2386</td>\n",
       "      <td>159</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>236</td>\n",
       "      <td>241</td>\n",
       "      <td>130</td>\n",
       "      <td>854</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581010</th>\n",
       "      <td>2384</td>\n",
       "      <td>170</td>\n",
       "      <td>15</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>230</td>\n",
       "      <td>245</td>\n",
       "      <td>143</td>\n",
       "      <td>864</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581011</th>\n",
       "      <td>2383</td>\n",
       "      <td>165</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>231</td>\n",
       "      <td>244</td>\n",
       "      <td>141</td>\n",
       "      <td>875</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581012 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0            2596      51      3                               258   \n",
       "1            2590      56      2                               212   \n",
       "2            2804     139      9                               268   \n",
       "3            2785     155     18                               242   \n",
       "4            2595      45      2                               153   \n",
       "...           ...     ...    ...                               ...   \n",
       "581007       2396     153     20                                85   \n",
       "581008       2391     152     19                                67   \n",
       "581009       2386     159     17                                60   \n",
       "581010       2384     170     15                                60   \n",
       "581011       2383     165     13                                60   \n",
       "\n",
       "        Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                                    0                              510   \n",
       "1                                   -6                              390   \n",
       "2                                   65                             3180   \n",
       "3                                  118                             3090   \n",
       "4                                   -1                              391   \n",
       "...                                ...                              ...   \n",
       "581007                              17                              108   \n",
       "581008                              12                               95   \n",
       "581009                               7                               90   \n",
       "581010                               5                               90   \n",
       "581011                               4                               67   \n",
       "\n",
       "        Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0                 221             232            148   \n",
       "1                 220             235            151   \n",
       "2                 234             238            135   \n",
       "3                 238             238            122   \n",
       "4                 220             234            150   \n",
       "...               ...             ...            ...   \n",
       "581007            240             237            118   \n",
       "581008            240             237            119   \n",
       "581009            236             241            130   \n",
       "581010            230             245            143   \n",
       "581011            231             244            141   \n",
       "\n",
       "        Horizontal_Distance_To_Fire_Points  ...  45  46  47  48  49  50  51  \\\n",
       "0                                     6279  ...   0   0   0   0   0   0   0   \n",
       "1                                     6225  ...   0   0   0   0   0   0   0   \n",
       "2                                     6121  ...   0   0   0   0   0   0   0   \n",
       "3                                     6211  ...   0   0   0   0   0   0   0   \n",
       "4                                     6172  ...   0   0   0   0   0   0   0   \n",
       "...                                    ...  ...  ..  ..  ..  ..  ..  ..  ..   \n",
       "581007                                 837  ...   0   0   0   0   0   0   0   \n",
       "581008                                 845  ...   0   0   0   0   0   0   0   \n",
       "581009                                 854  ...   0   0   0   0   0   0   0   \n",
       "581010                                 864  ...   0   0   0   0   0   0   0   \n",
       "581011                                 875  ...   0   0   0   0   0   0   0   \n",
       "\n",
       "        52  53  Cover Type  \n",
       "0        0   0           5  \n",
       "1        0   0           5  \n",
       "2        0   0           2  \n",
       "3        0   0           2  \n",
       "4        0   0           5  \n",
       "...     ..  ..         ...  \n",
       "581007   0   0           3  \n",
       "581008   0   0           3  \n",
       "581009   0   0           3  \n",
       "581010   0   0           3  \n",
       "581011   0   0           3  \n",
       "\n",
       "[581012 rows x 55 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covertype_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    283301\n",
       "1    211840\n",
       "3     35754\n",
       "7     20510\n",
       "6     17367\n",
       "5      9493\n",
       "4      2747\n",
       "Name: Cover Type, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see which class has the highest frequency and make that the positive class and the rest negative  \n",
    "covertype_df[\"Cover Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    297711\n",
       "1    283301\n",
       "Name: Cover Type, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binarize the cover type based on the highest frequency class \n",
    "covertype_df[\"Cover Type\"] = covertype_df[\"Cover Type\"].apply(lambda x: 1 if x == 2 else 0)\n",
    "# check the counts of the newly binarized column\n",
    "covertype_df[\"Cover Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>Cover Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581007</th>\n",
       "      <td>2396</td>\n",
       "      <td>153</td>\n",
       "      <td>20</td>\n",
       "      <td>85</td>\n",
       "      <td>17</td>\n",
       "      <td>108</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>118</td>\n",
       "      <td>837</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581008</th>\n",
       "      <td>2391</td>\n",
       "      <td>152</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>119</td>\n",
       "      <td>845</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581009</th>\n",
       "      <td>2386</td>\n",
       "      <td>159</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>236</td>\n",
       "      <td>241</td>\n",
       "      <td>130</td>\n",
       "      <td>854</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581010</th>\n",
       "      <td>2384</td>\n",
       "      <td>170</td>\n",
       "      <td>15</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>230</td>\n",
       "      <td>245</td>\n",
       "      <td>143</td>\n",
       "      <td>864</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581011</th>\n",
       "      <td>2383</td>\n",
       "      <td>165</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>231</td>\n",
       "      <td>244</td>\n",
       "      <td>141</td>\n",
       "      <td>875</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581012 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0            2596      51      3                               258   \n",
       "1            2590      56      2                               212   \n",
       "2            2804     139      9                               268   \n",
       "3            2785     155     18                               242   \n",
       "4            2595      45      2                               153   \n",
       "...           ...     ...    ...                               ...   \n",
       "581007       2396     153     20                                85   \n",
       "581008       2391     152     19                                67   \n",
       "581009       2386     159     17                                60   \n",
       "581010       2384     170     15                                60   \n",
       "581011       2383     165     13                                60   \n",
       "\n",
       "        Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                                    0                              510   \n",
       "1                                   -6                              390   \n",
       "2                                   65                             3180   \n",
       "3                                  118                             3090   \n",
       "4                                   -1                              391   \n",
       "...                                ...                              ...   \n",
       "581007                              17                              108   \n",
       "581008                              12                               95   \n",
       "581009                               7                               90   \n",
       "581010                               5                               90   \n",
       "581011                               4                               67   \n",
       "\n",
       "        Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0                 221             232            148   \n",
       "1                 220             235            151   \n",
       "2                 234             238            135   \n",
       "3                 238             238            122   \n",
       "4                 220             234            150   \n",
       "...               ...             ...            ...   \n",
       "581007            240             237            118   \n",
       "581008            240             237            119   \n",
       "581009            236             241            130   \n",
       "581010            230             245            143   \n",
       "581011            231             244            141   \n",
       "\n",
       "        Horizontal_Distance_To_Fire_Points  ...  45  46  47  48  49  50  51  \\\n",
       "0                                     6279  ...   0   0   0   0   0   0   0   \n",
       "1                                     6225  ...   0   0   0   0   0   0   0   \n",
       "2                                     6121  ...   0   0   0   0   0   0   0   \n",
       "3                                     6211  ...   0   0   0   0   0   0   0   \n",
       "4                                     6172  ...   0   0   0   0   0   0   0   \n",
       "...                                    ...  ...  ..  ..  ..  ..  ..  ..  ..   \n",
       "581007                                 837  ...   0   0   0   0   0   0   0   \n",
       "581008                                 845  ...   0   0   0   0   0   0   0   \n",
       "581009                                 854  ...   0   0   0   0   0   0   0   \n",
       "581010                                 864  ...   0   0   0   0   0   0   0   \n",
       "581011                                 875  ...   0   0   0   0   0   0   0   \n",
       "\n",
       "        52  53  Cover Type  \n",
       "0        0   0           0  \n",
       "1        0   0           0  \n",
       "2        0   0           1  \n",
       "3        0   0           1  \n",
       "4        0   0           0  \n",
       "...     ..  ..         ...  \n",
       "581007   0   0           0  \n",
       "581008   0   0           0  \n",
       "581009   0   0           0  \n",
       "581010   0   0           0  \n",
       "581011   0   0           0  \n",
       "\n",
       "[581012 rows x 55 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covertype_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in CalHousing dataset\n",
    "calhousing_df = pd.read_csv(\"data/cal_housing.data\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column names\n",
    "calhousing_df.columns = [\"Longitude\", \"Lattitude\", \"Housing Median Age\", \n",
    "                         \"Total Rooms\", \"Total Bedrooms\", \"Population\", \n",
    "                         \"Households\", \"Median Income\", \"Median House Val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    14728\n",
       "0     5912\n",
       "Name: Median House Val, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binarize output variable so that median housing value > $130,000 is 1 and <= $130,000 is 0\n",
    "calhousing_df[\"Median House Val\"] = calhousing_df[\"Median House Val\"].apply(lambda x: 1 \n",
    "                                                                            if x > 130000 else 0)\n",
    "# check the counts of the newly binarized column\n",
    "calhousing_df[\"Median House Val\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Housing Median Age</th>\n",
       "      <th>Total Rooms</th>\n",
       "      <th>Total Bedrooms</th>\n",
       "      <th>Population</th>\n",
       "      <th>Households</th>\n",
       "      <th>Median Income</th>\n",
       "      <th>Median House Val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>-121.09</td>\n",
       "      <td>39.48</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>1.5603</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>-121.21</td>\n",
       "      <td>39.49</td>\n",
       "      <td>18.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>2.5568</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>-121.22</td>\n",
       "      <td>39.43</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>1.7000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>-121.32</td>\n",
       "      <td>39.43</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>1.8672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>-121.24</td>\n",
       "      <td>39.37</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2785.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>2.3886</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Longitude  Lattitude  Housing Median Age  Total Rooms  Total Bedrooms  \\\n",
       "0        -122.23      37.88                41.0        880.0           129.0   \n",
       "1        -122.22      37.86                21.0       7099.0          1106.0   \n",
       "2        -122.24      37.85                52.0       1467.0           190.0   \n",
       "3        -122.25      37.85                52.0       1274.0           235.0   \n",
       "4        -122.25      37.85                52.0       1627.0           280.0   \n",
       "...          ...        ...                 ...          ...             ...   \n",
       "20635    -121.09      39.48                25.0       1665.0           374.0   \n",
       "20636    -121.21      39.49                18.0        697.0           150.0   \n",
       "20637    -121.22      39.43                17.0       2254.0           485.0   \n",
       "20638    -121.32      39.43                18.0       1860.0           409.0   \n",
       "20639    -121.24      39.37                16.0       2785.0           616.0   \n",
       "\n",
       "       Population  Households  Median Income  Median House Val  \n",
       "0           322.0       126.0         8.3252                 1  \n",
       "1          2401.0      1138.0         8.3014                 1  \n",
       "2           496.0       177.0         7.2574                 1  \n",
       "3           558.0       219.0         5.6431                 1  \n",
       "4           565.0       259.0         3.8462                 1  \n",
       "...           ...         ...            ...               ...  \n",
       "20635       845.0       330.0         1.5603                 0  \n",
       "20636       356.0       114.0         2.5568                 0  \n",
       "20637      1007.0       433.0         1.7000                 0  \n",
       "20638       741.0       349.0         1.8672                 0  \n",
       "20639      1387.0       530.0         2.3886                 0  \n",
       "\n",
       "[20640 rows x 9 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calhousing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry Beans Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the Dry Beans dataset\n",
    "beans_df = pd.read_csv(\"data/Dry_Beans_Dataset.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DERMASON    3546\n",
       "SIRA        2636\n",
       "SEKER       2027\n",
       "HOROZ       1928\n",
       "CALI        1630\n",
       "BARBUNYA    1322\n",
       "BOMBAY       522\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see which 2 classes have the highest frequency to make those the positive class \n",
    "beans_df[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7429\n",
       "1    6182\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarize the classes - make the two highest classes the postive class & the rest negative \n",
    "beans_df[\"Class\"] = beans_df[\"Class\"].apply(lambda x: 1 if x == \"DERMASON\" or x == \"SIRA\" else 0)\n",
    "# check the counts of the newly binarized column\n",
    "beans_df[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>AspectRation</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>roundness</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>ShapeFactor1</th>\n",
       "      <th>ShapeFactor2</th>\n",
       "      <th>ShapeFactor3</th>\n",
       "      <th>ShapeFactor4</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28395</td>\n",
       "      <td>610.291</td>\n",
       "      <td>208.178117</td>\n",
       "      <td>173.888747</td>\n",
       "      <td>1.197191</td>\n",
       "      <td>0.549812</td>\n",
       "      <td>28715</td>\n",
       "      <td>190.141097</td>\n",
       "      <td>0.763923</td>\n",
       "      <td>0.988856</td>\n",
       "      <td>0.958027</td>\n",
       "      <td>0.913358</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.834222</td>\n",
       "      <td>0.998724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28734</td>\n",
       "      <td>638.018</td>\n",
       "      <td>200.524796</td>\n",
       "      <td>182.734419</td>\n",
       "      <td>1.097356</td>\n",
       "      <td>0.411785</td>\n",
       "      <td>29172</td>\n",
       "      <td>191.272751</td>\n",
       "      <td>0.783968</td>\n",
       "      <td>0.984986</td>\n",
       "      <td>0.887034</td>\n",
       "      <td>0.953861</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.909851</td>\n",
       "      <td>0.998430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29380</td>\n",
       "      <td>624.110</td>\n",
       "      <td>212.826130</td>\n",
       "      <td>175.931143</td>\n",
       "      <td>1.209713</td>\n",
       "      <td>0.562727</td>\n",
       "      <td>29690</td>\n",
       "      <td>193.410904</td>\n",
       "      <td>0.778113</td>\n",
       "      <td>0.989559</td>\n",
       "      <td>0.947849</td>\n",
       "      <td>0.908774</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30008</td>\n",
       "      <td>645.884</td>\n",
       "      <td>210.557999</td>\n",
       "      <td>182.516516</td>\n",
       "      <td>1.153638</td>\n",
       "      <td>0.498616</td>\n",
       "      <td>30724</td>\n",
       "      <td>195.467062</td>\n",
       "      <td>0.782681</td>\n",
       "      <td>0.976696</td>\n",
       "      <td>0.903936</td>\n",
       "      <td>0.928329</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.861794</td>\n",
       "      <td>0.994199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30140</td>\n",
       "      <td>620.134</td>\n",
       "      <td>201.847882</td>\n",
       "      <td>190.279279</td>\n",
       "      <td>1.060798</td>\n",
       "      <td>0.333680</td>\n",
       "      <td>30417</td>\n",
       "      <td>195.896503</td>\n",
       "      <td>0.773098</td>\n",
       "      <td>0.990893</td>\n",
       "      <td>0.984877</td>\n",
       "      <td>0.970516</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.941900</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13606</th>\n",
       "      <td>42097</td>\n",
       "      <td>759.696</td>\n",
       "      <td>288.721612</td>\n",
       "      <td>185.944705</td>\n",
       "      <td>1.552728</td>\n",
       "      <td>0.765002</td>\n",
       "      <td>42508</td>\n",
       "      <td>231.515799</td>\n",
       "      <td>0.714574</td>\n",
       "      <td>0.990331</td>\n",
       "      <td>0.916603</td>\n",
       "      <td>0.801865</td>\n",
       "      <td>0.006858</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.642988</td>\n",
       "      <td>0.998385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13607</th>\n",
       "      <td>42101</td>\n",
       "      <td>757.499</td>\n",
       "      <td>281.576392</td>\n",
       "      <td>190.713136</td>\n",
       "      <td>1.476439</td>\n",
       "      <td>0.735702</td>\n",
       "      <td>42494</td>\n",
       "      <td>231.526798</td>\n",
       "      <td>0.799943</td>\n",
       "      <td>0.990752</td>\n",
       "      <td>0.922015</td>\n",
       "      <td>0.822252</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.676099</td>\n",
       "      <td>0.998219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13608</th>\n",
       "      <td>42139</td>\n",
       "      <td>759.321</td>\n",
       "      <td>281.539928</td>\n",
       "      <td>191.187979</td>\n",
       "      <td>1.472582</td>\n",
       "      <td>0.734065</td>\n",
       "      <td>42569</td>\n",
       "      <td>231.631261</td>\n",
       "      <td>0.729932</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.918424</td>\n",
       "      <td>0.822730</td>\n",
       "      <td>0.006681</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.676884</td>\n",
       "      <td>0.996767</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13609</th>\n",
       "      <td>42147</td>\n",
       "      <td>763.779</td>\n",
       "      <td>283.382636</td>\n",
       "      <td>190.275731</td>\n",
       "      <td>1.489326</td>\n",
       "      <td>0.741055</td>\n",
       "      <td>42667</td>\n",
       "      <td>231.653248</td>\n",
       "      <td>0.705389</td>\n",
       "      <td>0.987813</td>\n",
       "      <td>0.907906</td>\n",
       "      <td>0.817457</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.668237</td>\n",
       "      <td>0.995222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13610</th>\n",
       "      <td>42159</td>\n",
       "      <td>772.237</td>\n",
       "      <td>295.142741</td>\n",
       "      <td>182.204716</td>\n",
       "      <td>1.619841</td>\n",
       "      <td>0.786693</td>\n",
       "      <td>42600</td>\n",
       "      <td>231.686223</td>\n",
       "      <td>0.788962</td>\n",
       "      <td>0.989648</td>\n",
       "      <td>0.888380</td>\n",
       "      <td>0.784997</td>\n",
       "      <td>0.007001</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.616221</td>\n",
       "      <td>0.998180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13611 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
       "0      28395    610.291       208.178117       173.888747      1.197191   \n",
       "1      28734    638.018       200.524796       182.734419      1.097356   \n",
       "2      29380    624.110       212.826130       175.931143      1.209713   \n",
       "3      30008    645.884       210.557999       182.516516      1.153638   \n",
       "4      30140    620.134       201.847882       190.279279      1.060798   \n",
       "...      ...        ...              ...              ...           ...   \n",
       "13606  42097    759.696       288.721612       185.944705      1.552728   \n",
       "13607  42101    757.499       281.576392       190.713136      1.476439   \n",
       "13608  42139    759.321       281.539928       191.187979      1.472582   \n",
       "13609  42147    763.779       283.382636       190.275731      1.489326   \n",
       "13610  42159    772.237       295.142741       182.204716      1.619841   \n",
       "\n",
       "       Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  roundness  \\\n",
       "0          0.549812       28715     190.141097  0.763923  0.988856   0.958027   \n",
       "1          0.411785       29172     191.272751  0.783968  0.984986   0.887034   \n",
       "2          0.562727       29690     193.410904  0.778113  0.989559   0.947849   \n",
       "3          0.498616       30724     195.467062  0.782681  0.976696   0.903936   \n",
       "4          0.333680       30417     195.896503  0.773098  0.990893   0.984877   \n",
       "...             ...         ...            ...       ...       ...        ...   \n",
       "13606      0.765002       42508     231.515799  0.714574  0.990331   0.916603   \n",
       "13607      0.735702       42494     231.526798  0.799943  0.990752   0.922015   \n",
       "13608      0.734065       42569     231.631261  0.729932  0.989899   0.918424   \n",
       "13609      0.741055       42667     231.653248  0.705389  0.987813   0.907906   \n",
       "13610      0.786693       42600     231.686223  0.788962  0.989648   0.888380   \n",
       "\n",
       "       Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  \\\n",
       "0         0.913358      0.007332      0.003147      0.834222      0.998724   \n",
       "1         0.953861      0.006979      0.003564      0.909851      0.998430   \n",
       "2         0.908774      0.007244      0.003048      0.825871      0.999066   \n",
       "3         0.928329      0.007017      0.003215      0.861794      0.994199   \n",
       "4         0.970516      0.006697      0.003665      0.941900      0.999166   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "13606     0.801865      0.006858      0.001749      0.642988      0.998385   \n",
       "13607     0.822252      0.006688      0.001886      0.676099      0.998219   \n",
       "13608     0.822730      0.006681      0.001888      0.676884      0.996767   \n",
       "13609     0.817457      0.006724      0.001852      0.668237      0.995222   \n",
       "13610     0.784997      0.007001      0.001640      0.616221      0.998180   \n",
       "\n",
       "       Class  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "...      ...  \n",
       "13606      1  \n",
       "13607      1  \n",
       "13608      1  \n",
       "13609      1  \n",
       "13610      1  \n",
       "\n",
       "[13611 rows x 17 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four algorithms I will be training are Logistic Regression, Support Vector Machines (SVM), K Nearest Neighbors (KNN), and Random Forests (RF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X & y splits for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the X and y for all datasets\n",
    "\n",
    "# was originally using adult instead of beans, but now only using beans\n",
    "#X_adult = adult_df.iloc[:, :-1]\n",
    "#y_adult = adult_df.iloc[:, -1:]\n",
    "\n",
    "X_beans  = beans_df.iloc[:, :-1]\n",
    "y_beans = beans_df.iloc[:, -1:]\n",
    "\n",
    "X_letter = letter_df.iloc[:, 1:]\n",
    "y_letter = letter_df.iloc[:, 0]\n",
    "\n",
    "X_covertype = covertype_df.iloc[:, :-1]\n",
    "y_covertype = covertype_df.iloc[:, -1:]\n",
    "\n",
    "X_calhousing = calhousing_df.iloc[:, :-1]\n",
    "y_calhousing = calhousing_df.iloc[:, -1:]\n",
    "\n",
    "# create an array with all the X and y splits to pass through algorithm loops\n",
    "X_total = [X_beans, X_letter, X_covertype, X_calhousing]\n",
    "y_total = [y_beans, y_letter, y_covertype, y_calhousing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw test values\n",
      "AUC: [0.9871307469647064, 0.9857381429308004, 0.9869650302790988, 0.8966795174688855, 0.9863107723237451]\n",
      "Acc: [0.9466960864011148, 0.9440250841946348, 0.9499477412611775, 0.7841133433979793, 0.9484380443618627]\n",
      "F1: [0.94211123723042, 0.9399451781709445, 0.9455464308275426, 0.7664866222836327, 0.9435688866293849]\n",
      "train trial metrics: [0.971 0.918 0.912]\n",
      "test trial metrics: [0.969 0.915 0.908]\n",
      "Raw test values\n",
      "AUC: [0.8124646253359503, 0.8090583525898403, 0.8127006535506116, 0.8108140997392252, 0.8101219362748475]\n",
      "Acc: [0.7278, 0.7242666666666666, 0.7278666666666667, 0.7229333333333333, 0.7224666666666667]\n",
      "F1: [0.7292619852794907, 0.7271767810026385, 0.7315533342101802, 0.7231179213857429, 0.7237741357574149]\n",
      "train trial metrics: [0.818 0.732 0.735]\n",
      "test trial metrics: [0.811 0.725 0.727]\n",
      "Raw test values\n",
      "AUC: [0.7990187939641502, 0.7998214888174716, 0.7914906556858601, 0.7931409413303765, 0.7988772109564317]\n",
      "Acc: [0.7307556092581404, 0.7262140372075582, 0.7167142351201017, 0.71641215808004, 0.7310993520968313]\n",
      "F1: [0.7252286024473145, 0.7268721856598546, 0.7243508961602785, 0.7250278267400594, 0.7348082757487141]\n",
      "train trial metrics: [0.801 0.73  0.736]\n",
      "test trial metrics: [0.796 0.724 0.727]\n",
      "Raw test values\n",
      "AUC: [0.8908112553124239, 0.8915175930655257, 0.8879267443196939, 0.8899060754110683, 0.8890744264911231]\n",
      "Acc: [0.8307544757033248, 0.8338874680306906, 0.8294117647058824, 0.8312659846547314, 0.8297953964194373]\n",
      "F1: [0.8850380021715526, 0.8872108666406283, 0.8831259856316804, 0.8846002363135094, 0.884771881222405]\n",
      "train trial metrics: [0.893 0.83  0.884]\n",
      "test trial metrics: [0.89  0.831 0.885]\n",
      "Logistic Regression metrics train:\n",
      " [[0.9707294  0.91772    0.91199135]\n",
      " [0.81849992 0.73228    0.73480448]\n",
      " [0.80135044 0.73048    0.73638799]\n",
      " [0.89311518 0.83032    0.88398199]]\n",
      "Average Logistic Regression train metrics:\n",
      " [0.87092374 0.8027     0.81679145]\n",
      "Logistic Regression metrics test:\n",
      " [[0.96856484 0.91464406 0.90753167]\n",
      " [0.81103193 0.72506667 0.72697683]\n",
      " [0.79646982 0.72423908 0.72725756]\n",
      " [0.88984722 0.83102302 0.88494939]]\n",
      "Average Logistic Regression test metrics:\n",
      " [0.86647845 0.79874321 0.81167886]\n",
      "Average across Logistic Regression test metrics:\n",
      " [0.93024686 0.75435848 0.74932215 0.86860654]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model\n",
    "\n",
    "# store metrics of all 4 datasets \n",
    "lr_metrics_train = []\n",
    "lr_metrics_test = []\n",
    "for X, y in zip(X_total, y_total):\n",
    "    # create 3 lists for the AUC, Accuracy, and F1 Scores across the 5 trials for train & test\n",
    "    lr_AUC_train, lr_Acc_train, lr_F1_train = [], [], []\n",
    "    lr_AUC_test, lr_Acc_test, lr_F1_test = [], [], []\n",
    "    for trial in range(5):\n",
    "        # for each trial, randomly select 5000 samples for the training set & rest as test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000)\n",
    "        C_vals = [10**-8, 10**-7, 10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1, \n",
    "                  10**0, 10**1, 10**2, 10**3, 10**4]\n",
    "        # metrics to evaluate model on\n",
    "        scoring = {\"Accuracy\": make_scorer(accuracy_score), \"F1_Score\": \"f1\", \"AUC\": \"roc_auc\"}\n",
    "        # Create a pipeline\n",
    "        pipe = Pipeline([('std', StandardScaler()), \n",
    "                         ('classifier', LogisticRegression(max_iter = 2000))])\n",
    "        # Create search space of candidate learning algorithms and their hyperparameters\n",
    "        search_space = [{'classifier': [LogisticRegression(max_iter = 2000)],\n",
    "                     'classifier__C': C_vals, 'classifier__penalty': [\"l2\"]},\n",
    "                    {'classifier': [LogisticRegression()], 'classifier__penalty': [\"none\"]}]\n",
    "        # Grid Search with stratified 5 folds cross validation to find best hyperparmeters\n",
    "        search_results = GridSearchCV(pipe, search_space, scoring = scoring, refit = False, \n",
    "                                      cv=StratifiedKFold(n_splits = 5))\n",
    "        # train models\n",
    "        search_results.fit(X_train, y_train)\n",
    "\n",
    "        # find optimal hyperparameters for each metric\n",
    "        AUC_rank = search_results.cv_results_[\"rank_test_AUC\"]\n",
    "        # find index of #1 in the rank array to find best hyperparameter\n",
    "        AUC_ind = np.argmin(AUC_rank)\n",
    "        # save best hyperparameters \n",
    "        opt_AUC = search_results.cv_results_[\"params\"][AUC_ind]\n",
    "\n",
    "        Acc_rank = search_results.cv_results_[\"rank_test_Accuracy\"]\n",
    "        # find index of #1 in the array to find best hyperparameter\n",
    "        Acc_ind = np.argmin(Acc_rank)  \n",
    "        # save best hyperparameters \n",
    "        opt_Acc = search_results.cv_results_[\"params\"][Acc_ind]\n",
    "\n",
    "        F1_rank = search_results.cv_results_[\"rank_test_F1_Score\"]\n",
    "        # find index of #1 in the array to find best hyperparameter\n",
    "        F1_ind = np.argmin(F1_rank)\n",
    "        # save best hyperparameters \n",
    "        opt_F1 = search_results.cv_results_[\"params\"][F1_ind]\n",
    "        \n",
    "        # determine the optimal parameters for the three models \n",
    "        opt_models = [opt_AUC, opt_Acc, opt_F1]\n",
    "        # initialize models -- will now fill in with best parameters \n",
    "        lr_AUC_model =  LogisticRegression(max_iter = 2000)\n",
    "        lr_Acc_model = LogisticRegression(max_iter = 2000)\n",
    "        lr_F1_model =  LogisticRegression(max_iter = 2000)                                 \n",
    "        models = [lr_AUC_model, lr_Acc_model, lr_F1_model]\n",
    "        # create 3 optimal models each with best parameters for that metric\n",
    "        for opt, model in zip(opt_models, models):\n",
    "            if opt[\"classifier__penalty\"] == \"none\":\n",
    "                model = LogisticRegression(max_iter = 2000, penalty = opt[\"classifier__penalty\"])\n",
    "            else:\n",
    "                # l2 regularization \n",
    "                model = LogisticRegression(max_iter = 2000, C = opt[\"classifier__C\"],\n",
    "                                           penalty = opt[\"classifier__penalty\"])        \n",
    "        \n",
    "        # train 3 models with the optimal parameters -- one model for each metric\n",
    "        # AUC model\n",
    "        lr_AUC_model.fit(X_train, y_train)\n",
    "        # make predictions & calculate AUC on both training and testing sets \n",
    "        lr_AUC_pred_train = (lr_AUC_model.predict_proba(X_train)[:,1])\n",
    "        lr_AUC_score_train = roc_auc_score(y_train, lr_AUC_pred_train)\n",
    "        lr_AUC_pred_test = (lr_AUC_model.predict_proba(X_test)[:,1])\n",
    "        lr_AUC_score_test = roc_auc_score(y_test, lr_AUC_pred_test)\n",
    "        # add AUC for current trial \n",
    "        lr_AUC_train.append(lr_AUC_score_train)\n",
    "        lr_AUC_test.append(lr_AUC_score_test)\n",
    "        \n",
    "        # Accuracy model\n",
    "        lr_Acc_model.fit(X_train, y_train)\n",
    "        # calculate accuracy on both training and testing sets \n",
    "        lr_Acc_score_train = lr_Acc_model.score(X_train, y_train)\n",
    "        lr_Acc_score_test = lr_Acc_model.score(X_test, y_test)\n",
    "        # add Accuracy for current trial \n",
    "        lr_Acc_train.append(lr_Acc_score_train)\n",
    "        lr_Acc_test.append(lr_Acc_score_test)\n",
    "\n",
    "        # F1 score model\n",
    "        lr_F1_model.fit(X_train, y_train)\n",
    "        # make predictions & calculate F1 score on both training and testing sets \n",
    "        lr_F1_pred_train = (lr_F1_model.predict(X_train))\n",
    "        lr_F1_score_train = f1_score(y_train, lr_F1_pred_train)\n",
    "        lr_F1_pred_test = (lr_F1_model.predict(X_test))\n",
    "        lr_F1_score_test = f1_score(y_test, lr_F1_pred_test)\n",
    "        # add F1 score for current trial \n",
    "        lr_F1_train.append(lr_F1_score_train)\n",
    "        lr_F1_test.append(lr_F1_score_test)\n",
    "        \n",
    "    # average train & test AUC, Accuracy, and F1 score across all 5 trials \n",
    "    lr_AUC_train_m, lr_AUC_test_m = np.mean(lr_AUC_train), np.mean(lr_AUC_test)\n",
    "    lr_Acc_train_m, lr_Acc_test_m = np.mean(lr_Acc_train), np.mean(lr_Acc_test)\n",
    "    lr_F1_train_m, lr_F1_test_m = np.mean(lr_F1_train), np.mean(lr_F1_test)\n",
    "    # combine average training metrics into one array for current trial \n",
    "    lr_trial_metrics_train = [lr_AUC_train_m, lr_Acc_train_m, lr_F1_train_m]\n",
    "    # combine average testing metrics into one array for current trial \n",
    "    lr_trial_metrics_test = [lr_AUC_test_m, lr_Acc_test_m, lr_F1_test_m]\n",
    "    # add average train and test metrics for current trial\n",
    "    lr_metrics_train.append(lr_trial_metrics_train)\n",
    "    lr_metrics_test.append(lr_trial_metrics_test)\n",
    "\n",
    "    # print raw test metric values for current trial\n",
    "    print(\"Raw test values\")\n",
    "    print(\"AUC:\", lr_AUC_test)\n",
    "    print(\"Acc:\", lr_Acc_test)\n",
    "    print(\"F1:\", lr_F1_test)\n",
    "    # print average metrics for train and test for current trial\n",
    "    print(\"train trial metrics:\", np.round(lr_trial_metrics_train, 3))\n",
    "    print(\"test trial metrics:\", np.round(lr_trial_metrics_test, 3))\n",
    "\n",
    "# final metrics from all 4 datasets\n",
    "lr_metrics_train = np.array(lr_metrics_train)\n",
    "print(\"Logistic Regression metrics train:\\n\", lr_metrics_train)\n",
    "lr_metrics_train_m = np.mean(lr_metrics_train, axis=0)\n",
    "print(\"Average Logistic Regression train metrics:\\n\", lr_metrics_train_m)\n",
    "lr_metrics_test = np.array(lr_metrics_test)\n",
    "print(\"Logistic Regression metrics test:\\n\", lr_metrics_test)\n",
    "lr_metrics_test_m = np.mean(lr_metrics_test, axis=0)\n",
    "print(\"Average Logistic Regression test metrics:\\n\", lr_metrics_test_m)\n",
    "# calculate average across metrics \n",
    "lr_metrics_test_m_2 = np.mean(lr_metrics_test, axis=1)\n",
    "print(\"Average across Logistic Regression test metrics:\\n\", lr_metrics_test_m_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the full arrays for the lr values in both tables 2 and 3, and will be used to calculate the t tests and p values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 2 lr arrays (cols of Lr metrics test)\n",
    "lr_auc = [0.96856484, 0.81103193, 0.79646982, 0.88984722]\n",
    "lr_acc = [0.91464406, 0.72506667, 0.72423908, 0.83102302]\n",
    "lr_f1 = [0.90753167, 0.72697683, 0.72725756, 0.88494939]\n",
    "lr_mean_1 = [0.86647845, 0.79874321, 0.81167886]\n",
    "\n",
    "# table 3 lr arrays (averaged AUC, ACC, and F1 metrics for all trials per dataset)\n",
    "lr_d1 = [0.9586460235320803, 0.9565694684321265, 0.9608197341226062, \n",
    "         0.8157598277168324, 0.959439234438331]\n",
    "lr_d2 = [0.7565088702051469, 0.7535006000863818, 0.7573735514758195, \n",
    "         0.7522884514861006, 0.752120912899643]\n",
    "lr_d3 = [0.751667668556535, 0.7509692372282948, 0.74418526232208, \n",
    "         0.7448603087168252, 0.7549282796006591]\n",
    "lr_d4 = [0.8688679110624338, 0.8708719759122815, 0.8668214982190855, \n",
    "         0.8685907654597697, 0.8678805680443218]\n",
    "lr_mean_2 = [0.93024686, 0.75435848, 0.74932215, 0.86860654]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial: 0\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "Raw test values\n",
      "AUC: [0.8604035759252292, 0.8639090242455969, 0.8660821741915149, 0.8611989958814702, 0.8638330991827154]\n",
      "Acc: [0.7823713854372315, 0.7813262106607827, 0.784693996051562, 0.7799326442921845, 0.7858553013587272]\n",
      "F1: [0.7924695459579181, 0.7892085525579312, 0.7922456297624383, 0.7893274041133963, 0.7954747116237799]\n",
      "train trial metrics: [0.867 0.788 0.797]\n",
      "test trial metrics: [0.863 0.783 0.792]\n",
      "trial: 0\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "Raw test values\n",
      "AUC: [0.9524253553565456, 0.9531144230829234, 0.9511932994979699, 0.9516859665068053, 0.9512504766631635]\n",
      "Acc: [0.8795333333333333, 0.8789333333333333, 0.8767333333333334, 0.8803333333333333, 0.8812666666666666]\n",
      "F1: [0.8786678305244076, 0.8792392605399655, 0.8755301245371928, 0.8803253550236683, 0.8790984997624057]\n",
      "train trial metrics: [0.96  0.89  0.888]\n",
      "test trial metrics: [0.952 0.879 0.879]\n",
      "trial: 0\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "Raw test values\n",
      "AUC: [0.7562672149943853, 0.7579740737064173, 0.751593227687614, 0.7517994243494166, 0.7553666286699963]\n",
      "Acc: [0.6935549953820407, 0.6978847662895912, 0.6916123275209545, 0.6946435143712284, 0.6957493941098449]\n",
      "F1: [0.6768857908533104, 0.6939217734814987, 0.6760264890999651, 0.6835179743883634, 0.6814943514735602]\n",
      "train trial metrics: [0.763 0.703 0.688]\n",
      "test trial metrics: [0.755 0.695 0.682]\n",
      "trial: 0\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "Raw test values\n",
      "AUC: [0.8253671841943817, 0.7514659022459358, 0.8064260426450938, 0.7844186635179224, 0.7867515489854504]\n",
      "Acc: [0.7119565217391305, 0.7125319693094629, 0.7118925831202046, 0.7140664961636829, 0.7162404092071611]\n",
      "F1: [0.8317460317460318, 0.8321135175504107, 0.8317023978486591, 0.8331841241420471, 0.8346620967141049]\n",
      "train trial metrics: [0.815 0.714 0.833]\n",
      "test trial metrics: [0.791 0.713 0.833]\n",
      "SVM metrics train:\n",
      " [[0.8669318  0.78764    0.79741423]\n",
      " [0.9600166  0.88956    0.88776708]\n",
      " [0.76303965 0.70272    0.68767883]\n",
      " [0.81530286 0.714      0.83306661]]\n",
      "Average SVM train metrics:\n",
      " [0.85132273 0.77348    0.80148168]\n",
      "SVM metrics test:\n",
      " [[0.86308537 0.78283591 0.79174517]\n",
      " [0.9519339  0.87936    0.87857221]\n",
      " [0.75460011 0.694689   0.68236928]\n",
      " [0.79088587 0.7133376  0.83268163]]\n",
      "Average SVM test metrics:\n",
      " [0.84012632 0.76755563 0.79634207]\n",
      "Average across SVM test metrics:\n",
      " [0.81255548 0.90328871 0.7105528  0.77896837]\n"
     ]
    }
   ],
   "source": [
    "# SVM model \n",
    "# store metrics of all 4 datasets \n",
    "svm_metrics_train = []\n",
    "svm_metrics_test = []\n",
    "for X, y in zip(X_total, y_total):\n",
    "    # create 3 lists for the AUC, Accuracy, and F1 Scores across the 5 trials\n",
    "    svm_AUC_train, svm_Acc_train, svm_F1_train = [], [], []\n",
    "    svm_AUC_test, svm_Acc_test, svm_F1_test = [], [], []\n",
    "    for trial in range(5):\n",
    "        print(\"trial:\", trial)\n",
    "        # for each trial, randomly select 5000 samples for the training set & rest as test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000) \n",
    "        # hyperparameters\n",
    "        C_vals = [10**-7, 10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1, \n",
    "                   10**0, 10**1, 10**2, 10**3]\n",
    "        degree = [2, 3]\n",
    "        gamma = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2]\n",
    "        # metrics to evaluate model on\n",
    "        scoring = {\"Accuracy\": make_scorer(accuracy_score), \"F1_Score\": \"f1\", \"AUC\": \"roc_auc\"}\n",
    "        # Create a pipeline\n",
    "        pipe = Pipeline([('std', StandardScaler()), ('classifier', SVC(probability=True))])\n",
    "    \n",
    "        # Create search space of candidate learning algorithms and their hyperparameters\n",
    "        search_space = [{'classifier': [SVC(probability=True)], 'classifier__C': C_vals, \n",
    "                        'classifier__kernel': [\"linear\"]},\n",
    "                        {'classifier': [SVC(probability=True)], 'classifier__C': C_vals, \n",
    "                         'classifier__kernel': [\"poly\"],'classifier__degree': degree},\n",
    "                        {'classifier': [SVC(probability=True)], 'classifier__C': C_vals, \n",
    "                        'classifier__kernel': [\"rbf\"], 'classifier__gamma': gamma}]\n",
    "        # Grid Search with stratified 5 folds cross validation to find best hyperparmeters\n",
    "        search_results = GridSearchCV(pipe, search_space, scoring = scoring, refit = False, \n",
    "                                      cv=StratifiedKFold(n_splits = 5))\n",
    "        # train models\n",
    "        search_results.fit(X_train, y_train)\n",
    "\n",
    "        # find optimal hyperparameters for each metric\n",
    "        AUC_rank = search_results.cv_results_[\"rank_test_AUC\"]\n",
    "        # find index of #1 in the rank array to find best hyperparameter\n",
    "        AUC_ind = np.argmin(AUC_rank)\n",
    "        # save best hyperparameters \n",
    "        opt_AUC = search_results.cv_results_[\"params\"][AUC_ind]\n",
    "\n",
    "        Acc_rank = search_results.cv_results_[\"rank_test_Accuracy\"]\n",
    "        # find index of #1 in the rank array to find best hyperparameter\n",
    "        Acc_ind = np.argmin(Acc_rank) \n",
    "        # save best hyperparameters \n",
    "        opt_Acc = search_results.cv_results_[\"params\"][Acc_ind]\n",
    "\n",
    "        F1_rank = search_results.cv_results_[\"rank_test_F1_Score\"]\n",
    "        # find index of #1 in the rank array to find best hyperparameter\n",
    "        F1_ind = np.argmin(F1_rank)\n",
    "        # save best hyperparameters \n",
    "        opt_F1 = search_results.cv_results_[\"params\"][F1_ind]\n",
    "\n",
    "        # determine the optimal parameters for the three models \n",
    "        opt_models = [opt_AUC, opt_Acc, opt_F1]\n",
    "        # initialize models -- will now fill in with best parameters \n",
    "        svm_AUC_model = SVC(probability=True)\n",
    "        svm_Acc_model = SVC(probability=True)\n",
    "        svm_F1_model, = SVC(probability=True)\n",
    "        models = [svm_AUC_model, svm_Acc_model, svm_F1_model]\n",
    "        # create 3 optimal models each with best parameters for that metric\n",
    "        for opt, model in zip(opt_models, models):\n",
    "            if opt[\"classifier__kernel\"] == \"poly\":\n",
    "                model = SVC(kernel = opt[\"classifier__kernel\"], C = opt[\"classifier__C\"],\n",
    "                            degree = opt[\"classifier__degree\"], probability=True)\n",
    "            elif opt[\"classifier__kernel\"] == \"gamma\":\n",
    "                model = SVC(kernel = opt[\"classifier__kernel\"], C = opt[\"classifier__C\"],\n",
    "                            gamma = opt[\"classifier__gamma\"], probability=True)\n",
    "            else:\n",
    "                model = SVC(kernel = opt[\"classifier__kernel\"], C = opt[\"classifier__C\"], \n",
    "                            probability=True)\n",
    "            \n",
    "        # train 3 models with the optimal parameters -- one model for each metric\n",
    "        # AUC model\n",
    "        svm_AUC_model.fit(X_train, y_train)\n",
    "        # make predictions & calculate AUC on both training and testing sets \n",
    "        svm_AUC_pred_train = (svm_AUC_model.predict_proba(X_train)[:,1])\n",
    "        svm_AUC_score_train = roc_auc_score(y_train, svm_AUC_pred_train)\n",
    "        svm_AUC_pred_test = (svm_AUC_model.predict_proba(X_test)[:,1])\n",
    "        svm_AUC_score_test = roc_auc_score(y_test, svm_AUC_pred_test)\n",
    "        # add AUC for current trial \n",
    "        svm_AUC_train.append(svm_AUC_score_train)\n",
    "        svm_AUC_test.append(svm_AUC_score_test)\n",
    "        \n",
    "        # Accuracy model\n",
    "        svm_Acc_model.fit(X_train, y_train)\n",
    "        # calculate accuracy on both training and testing sets \n",
    "        svm_Acc_score_train = svm_Acc_model.score(X_train, y_train)\n",
    "        svm_Acc_score_test = svm_Acc_model.score(X_test, y_test)\n",
    "        # add Accuracy for current trial \n",
    "        svm_Acc_train.append(svm_Acc_score_train)\n",
    "        svm_Acc_test.append(svm_Acc_score_test)\n",
    "\n",
    "        # F1 score model\n",
    "        svm_F1_model.fit(X_train, y_train)\n",
    "        # make predictions & calculate F1 score on both training and testing sets \n",
    "        svm_F1_pred_train = (svm_F1_model.predict(X_train))\n",
    "        svm_F1_score_train = f1_score(y_train, svm_F1_pred_train)\n",
    "        svm_F1_pred_test = (svm_F1_model.predict(X_test))\n",
    "        svm_F1_score_test = f1_score(y_test, svm_F1_pred_test)\n",
    "        # add F1 score for current trial \n",
    "        svm_F1_train.append(svm_F1_score_train)\n",
    "        svm_F1_test.append(svm_F1_score_test)\n",
    "    \n",
    "    # average AUC, Accuracy, and F1 score across all 5 trials \n",
    "    svm_AUC_train_m, svm_AUC_test_m = np.mean(svm_AUC_train), np.mean(svm_AUC_test)\n",
    "    svm_Acc_train_m, svm_Acc_test_m = np.mean(svm_Acc_train), np.mean(svm_Acc_test)\n",
    "    svm_F1_train_m, svm_F1_test_m = np.mean(svm_F1_train), np.mean(svm_F1_test)     \n",
    "    # combine average training metrics into one array for current trial\n",
    "    svm_trial_metrics_train = [svm_AUC_train_m, svm_Acc_train_m, svm_F1_train_m]\n",
    "    # combine average testing metrics into one array for current trial\n",
    "    svm_trial_metrics_test = [svm_AUC_test_m, svm_Acc_test_m, svm_F1_test_m]\n",
    "    # add average train and test metrics for current trial\n",
    "    svm_metrics_train.append(svm_trial_metrics_train)\n",
    "    svm_metrics_test.append(svm_trial_metrics_test)\n",
    "    \n",
    "    # print raw test metric values for current trial\n",
    "    print(\"Raw test values\")\n",
    "    print(\"AUC:\", svm_AUC_test)\n",
    "    print(\"Acc:\", svm_Acc_test)\n",
    "    print(\"F1:\", svm_F1_test)\n",
    "    # print average metrics for train and test for current trial\n",
    "    print(\"train trial metrics:\", np.round(svm_trial_metrics_train, 3))\n",
    "    print(\"test trial metrics:\", np.round(svm_trial_metrics_test, 3))\n",
    "\n",
    "# final metrics from all 4 datasets\n",
    "svm_metrics_train = np.array(svm_metrics_train)\n",
    "print(\"SVM metrics train:\\n\", svm_metrics_train)\n",
    "svm_metrics_train_m = np.mean(svm_metrics_train, axis=0)\n",
    "print(\"Average SVM train metrics:\\n\", svm_metrics_train_m) \n",
    "svm_metrics_test = np.array(svm_metrics_test)\n",
    "print(\"SVM metrics test:\\n\", svm_metrics_test)\n",
    "svm_metrics_test_m = np.mean(svm_metrics_test, axis=0)\n",
    "print(\"Average SVM test metrics:\\n\", svm_metrics_test_m)\n",
    "# calculate average across metrics \n",
    "svm_metrics_test_m_2 = np.mean(svm_metrics_test, axis=1)\n",
    "print(\"Average across SVM test metrics:\\n\", svm_metrics_test_m_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the full arrays for the svm values in both tables 2 and 3, and will be used to calculate the t tests and p values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 2 svm arrays (cols of svm metrics test)\n",
    "svm_auc = [0.86308537, 0.9519339, 0.75460011, 0.79088587]\n",
    "svm_acc = [0.78283591, 0.87936, 0.694689, 0.7133376]\n",
    "svm_f1 = [0.79174517, 0.87857221, 0.68236928, 0.83268163]\n",
    "svm_mean_1 = [0.84012632, 0.76755563, 0.79634207]\n",
    "\n",
    "# table 3 svm arrays (averaged AUC, ACC, and F1 metrics for all trials per dataset)\n",
    "svm_d1 = [0.8117481691067928, 0.8114812624881037, 0.8143406000018384, \n",
    "          0.8101530147623502, 0.8150543707217408]\n",
    "svm_d2 = [0.9035421730714287, 0.9037623389854074, 0.9011522524561654, \n",
    "          0.9041148849546022, 0.9038718810307453]\n",
    "svm_d3 = [0.7089026670765787, 0.7165935378258358, 0.7064106814361778, \n",
    "          0.7099869710363361, 0.7108701247511338]\n",
    "svm_d4 = [0.7896899125598481, 0.7653704630352699, 0.7833403412046526, \n",
    "          0.7772230946078841, 0.7792180183022389]\n",
    "svm_mean_2 = [0.81255548, 0.90328871, 0.7105528, 0.77896837]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial: 0\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "Raw test values\n",
      "AUC: [0.9186584716544333, 0.9107685411788818, 0.9117882994902321, 0.9232789366433118, 0.9164194998838217]\n",
      "Acc: [0.8418302171640925, 0.7989780513296946, 0.8409011729183602, 0.8324236441760539, 0.8494948321913831]\n",
      "F1: [0.8347087378640777, 0.7993508751593833, 0.8375622480436329, 0.8277837450769783, 0.8423741182194114]\n",
      "train trial metrics: [1.0, 0.91544, 0.9115126875396428]\n",
      "test trial metrics: [0.916 0.833 0.828]\n",
      "trial: 0\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "Raw test values\n",
      "AUC: [0.9912599000264377, 0.9901761003301686, 0.9903323285507352, 0.9904655045553582, 0.9907913247358082]\n",
      "Acc: [0.959, 0.9572666666666667, 0.9574666666666667, 0.9575333333333333, 0.9560666666666666]\n",
      "F1: [0.9590355025644441, 0.9572238905572238, 0.9571697099892588, 0.9576490924805532, 0.9557629052829428]\n",
      "train trial metrics: [1.0, 1.0, 1.0]\n",
      "test trial metrics: [0.991 0.957 0.957]\n",
      "trial: 0\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "Raw test values\n",
      "AUC: [0.8588672610437202, 0.8585651485056803, 0.8612744085839801, 0.8605461430320636, 0.8611968069327682]\n",
      "Acc: [0.7768154135677728, 0.779607022075929, 0.7820392630709082, 0.7792702235370097, 0.7852405852655847]\n",
      "F1: [0.7787280697979831, 0.7772639879286427, 0.7762673927919708, 0.7754759796034943, 0.7838279906613915]\n",
      "train trial metrics: [1.0, 1.0, 1.0]\n",
      "test trial metrics: [0.86  0.781 0.778]\n",
      "trial: 0\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "Raw test values\n",
      "AUC: [0.6697572508645547, 0.6661878008514497, 0.6680496603664368, 0.6648510068387052, 0.6543621391769073]\n",
      "Acc: [0.6968030690537085, 0.6914322250639386, 0.6792838874680307, 0.6925191815856777, 0.6956521739130435]\n",
      "F1: [0.8127480457005413, 0.8023751023751022, 0.8121778350515463, 0.8038343871099327, 0.8112259257770095]\n",
      "train trial metrics: [1.0, 0.9513999999999999, 0.9688661114670083]\n",
      "test trial metrics: [0.665 0.691 0.808]\n",
      "KNN metrics train:\n",
      " [[1.         0.91544    0.91151269]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         0.9514     0.96886611]]\n",
      "Average KNN train metrics:\n",
      " [1.        0.96671   0.9700947]\n",
      "KNN metrics test:\n",
      " [[0.91618275 0.83272558 0.82835594]\n",
      " [0.99060503 0.95746667 0.95736822]\n",
      " [0.86008995 0.7805945  0.77831268]\n",
      " [0.66464157 0.69113811 0.80847226]]\n",
      "Average KNN test metrics:\n",
      " [0.85787983 0.81548121 0.84312728]\n",
      "Average across KNN test metrics:\n",
      " [0.85908809 0.96847997 0.80633238 0.72141731]\n"
     ]
    }
   ],
   "source": [
    "# KNN model \n",
    "# store metrics of all 4 datasets \n",
    "knn_metrics_train = []\n",
    "knn_metrics_test = []\n",
    "for X, y in zip(X_total, y_total):\n",
    "    # create 3 lists for the AUC, Accuracy, and F1 Scores across the 5 trials\n",
    "    knn_AUC_train, knn_Acc_train, knn_F1_train = [], [], []\n",
    "    knn_AUC_test, knn_Acc_test, knn_F1_test = [], [], []\n",
    "    for trial in range(5):\n",
    "        print(\"trial:\", trial)\n",
    "        # for each trial, randomly select 5000 samples for the training set & rest as test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000) \n",
    "        # hyperparameters\n",
    "        n_neighbors = [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57,\n",
    "                       61, 65, 69, 73, 77, 81, 85, 89, 93, 97, 101]\n",
    "        weights = [\"uniform\", \"distance\"]\n",
    "        algorithms = [\"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "        # metrics to evaluate model on\n",
    "        scoring = {\"Accuracy\": make_scorer(accuracy_score), \"F1_Score\": \"f1\", \"AUC\": \"roc_auc\"}\n",
    "\n",
    "        # Create a pipeline\n",
    "        pipe = Pipeline([('std', StandardScaler()), \n",
    "                         ('classifier', KNeighborsClassifier(metric=\"euclidean\"))])\n",
    "        # Create search space of candidate learning algorithms and their hyperparameters\n",
    "        search_space = [{'classifier': [KNeighborsClassifier(metric=\"euclidean\")], \n",
    "                         'classifier__n_neighbors': n_neighbors, 'classifier__weights': weights, \n",
    "                         'classifier__algorithm': algorithms}]\n",
    "        # Grid Search with stratified 5 folds cross validation to find best hyperparmeters\n",
    "        search_results = GridSearchCV(pipe, search_space, scoring = scoring, refit = False, \n",
    "                                      cv=StratifiedKFold(n_splits = 5))\n",
    "        # train models\n",
    "        search_results.fit(X_train, y_train)\n",
    "\n",
    "        # find optimal hyperparameters for each metric\n",
    "        AUC_rank = search_results.cv_results_[\"rank_test_AUC\"]\n",
    "        # find index of #1 in the rank array to find best hyperparameter\n",
    "        AUC_ind = np.argmin(AUC_rank)\n",
    "        # save best hyperparameters\n",
    "        opt_AUC = search_results.cv_results_[\"params\"][AUC_ind]\n",
    "\n",
    "        Acc_rank = search_results.cv_results_[\"rank_test_Accuracy\"]\n",
    "        # find index of #1 in the rank array to find best hyperparameter\n",
    "        Acc_ind = np.argmin(Acc_rank) \n",
    "        # save best hyperparameters\n",
    "        opt_Acc = search_results.cv_results_[\"params\"][Acc_ind]\n",
    "\n",
    "        F1_rank = search_results.cv_results_[\"rank_test_F1_Score\"]\n",
    "        # find index of #1 in the rank array to find best hyperparameter\n",
    "        F1_ind = np.argmin(F1_rank)\n",
    "        # save best hyperparameters\n",
    "        opt_F1 = search_results.cv_results_[\"params\"][F1_ind]\n",
    "\n",
    "        # train 3 models with the optimal parameters -- one model for each metric\n",
    "        # AUC model\n",
    "        knn_AUC_model = KNeighborsClassifier(metric=\"euclidean\", \n",
    "                                             n_neighbors = opt_AUC[\"classifier__n_neighbors\"], \n",
    "                                             weights = opt_AUC[\"classifier__weights\"], \n",
    "                                             algorithm = opt_AUC[\"classifier__algorithm\"])\n",
    "        knn_AUC_model.fit(X_train, y_train)\n",
    "        # make predictions & calculate AUC on both training and testing sets\n",
    "        knn_AUC_pred_train = (knn_AUC_model.predict_proba(X_train)[:,1])\n",
    "        knn_AUC_score_train = roc_auc_score(y_train, knn_AUC_pred_train)\n",
    "        knn_AUC_pred_test = (knn_AUC_model.predict_proba(X_test)[:,1])\n",
    "        knn_AUC_score_test = roc_auc_score(y_test, knn_AUC_pred_test)\n",
    "        # add AUC for current trial \n",
    "        knn_AUC_train.append(knn_AUC_score_train)\n",
    "        knn_AUC_test.append(knn_AUC_score_test)\n",
    "\n",
    "        # Accuracy model\n",
    "        knn_Acc_model = KNeighborsClassifier(metric=\"euclidean\", \n",
    "                                             n_neighbors = opt_Acc[\"classifier__n_neighbors\"], \n",
    "                                             weights = opt_Acc[\"classifier__weights\"], \n",
    "                                             algorithm = opt_Acc[\"classifier__algorithm\"])\n",
    "        knn_Acc_model.fit(X_train, y_train)\n",
    "        # calculate accuracy on both training and testing sets \n",
    "        knn_Acc_score_train = knn_Acc_model.score(X_train, y_train)\n",
    "        knn_Acc_score_test = knn_Acc_model.score(X_test, y_test)\n",
    "        # add Accuracy for current trial \n",
    "        knn_Acc_train.append(knn_Acc_score_train)\n",
    "        knn_Acc_test.append(knn_Acc_score_test)\n",
    "\n",
    "        # F1 Score model\n",
    "        knn_F1_model = KNeighborsClassifier(metric=\"euclidean\", \n",
    "                                            n_neighbors = opt_F1[\"classifier__n_neighbors\"], \n",
    "                                            weights = opt_F1[\"classifier__weights\"], \n",
    "                                            algorithm = opt_F1[\"classifier__algorithm\"])\n",
    "        knn_F1_model.fit(X_train, y_train)\n",
    "        # make predictions & calculate F1 score on both training and testing sets \n",
    "        knn_F1_pred_train = (knn_F1_model.predict(X_train))\n",
    "        knn_F1_score_train = f1_score(y_train, knn_F1_pred_train)\n",
    "        knn_F1_pred_test = (knn_F1_model.predict(X_test))\n",
    "        knn_F1_score_test = f1_score(y_test, knn_F1_pred_test)\n",
    "        # add F1 score for current trial \n",
    "        knn_F1_train.append(knn_F1_score_train)\n",
    "        knn_F1_test.append(knn_F1_score_test)\n",
    "    \n",
    "    # average AUC, Accuracy, and F1 score across all 5 trials \n",
    "    knn_AUC_train_m, knn_AUC_test_m = np.mean(knn_AUC_train), np.mean(knn_AUC_test)\n",
    "    knn_Acc_train_m, knn_Acc_test_m = np.mean(knn_Acc_train), np.mean(knn_Acc_test)\n",
    "    knn_F1_train_m, knn_F1_test_m = np.mean(knn_F1_train), np.mean(knn_F1_test)     \n",
    "    # combine average training metrics into one array for current trial\n",
    "    knn_trial_metrics_train = [knn_AUC_train_m, knn_Acc_train_m, knn_F1_train_m]\n",
    "    # combine average testing metrics into one array for current trial\n",
    "    knn_trial_metrics_test = [knn_AUC_test_m, knn_Acc_test_m, knn_F1_test_m]\n",
    "    # add average train and test metrics for current trial\n",
    "    knn_metrics_train.append(knn_trial_metrics_train)\n",
    "    knn_metrics_test.append(knn_trial_metrics_test)\n",
    "    \n",
    "    # print raw test metric values for current trial\n",
    "    print(\"Raw test values\")\n",
    "    print(\"AUC:\", knn_AUC_test)\n",
    "    print(\"Acc:\", knn_Acc_test)\n",
    "    print(\"F1:\", knn_F1_test)\n",
    "    # print average metrics for train and test for current trial\n",
    "    print(\"train trial metrics:\", knn_trial_metrics_train)\n",
    "    print(\"test trial metrics:\", np.round(knn_trial_metrics_test, 3))\n",
    "\n",
    "# final metrics from all 4 datasets\n",
    "knn_metrics_train = np.array(knn_metrics_train)\n",
    "print(\"KNN metrics train:\\n\", knn_metrics_train)\n",
    "knn_metrics_train_m = np.mean(knn_metrics_train, axis=0)\n",
    "print(\"Average KNN train metrics:\\n\", knn_metrics_train_m) \n",
    "knn_metrics_test = np.array(knn_metrics_test)\n",
    "print(\"KNN metrics test:\\n\", knn_metrics_test)\n",
    "knn_metrics_test_m = np.mean(knn_metrics_test, axis=0)\n",
    "print(\"Average KNN test metrics:\\n\", knn_metrics_test_m)\n",
    "# calculate average across metrics \n",
    "knn_metrics_test_m_2 = np.mean(knn_metrics_test, axis=1)\n",
    "print(\"Average across KNN test metrics:\\n\", knn_metrics_test_m_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the full arrays for the knn values in both tables 2 and 3, and will be used to calculate the t tests and p values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 2 knn arrays (cols of knn metrics test)\n",
    "knn_auc = [0.91618275, 0.99060503, 0.86008995, 0.66464157]\n",
    "knn_acc = [0.83272558, 0.95746667, 0.7805945, 0.69113811]\n",
    "knn_f1 = [0.82835594, 0.95736822, 0.77831268, 0.80847226]\n",
    "knn_mean_1 = [0.85787983, 0.81548121, 0.84312728]\n",
    "\n",
    "# table 3 knn arrays (averaged AUC, ACC, and F1 metrics for all trials per dataset)\n",
    "knn_d1 = [0.8650658088942013, 0.8363658225559866, 0.8634172401507417, \n",
    "          0.8611621086321147, 0.8694294834315387]\n",
    "knn_d2 = [0.9697651341969605, 0.9682222191846863, 0.9683229017355536, \n",
    "          0.9685493101230817, 0.9675402988951393]\n",
    "knn_d3 = [0.8048035814698253, 0.8051453861700839, 0.8065270214822863, \n",
    "          0.8050974487241892, 0.8100884609532483]\n",
    "knn_d4 = [0.7264361218729348, 0.7199983760968302, 0.7198371276286712, \n",
    "          0.7204015251781053, 0.7204134129556534]\n",
    "knn_mean_2 = [0.85908809, 0.96847997, 0.80633238, 0.72141731]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial: 0\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "Raw test values\n",
      "AUC: [0.9956086459416813, 0.9955518649409411, 0.9953060760871978, 0.9953704261712993, 0.9960015003015847]\n",
      "Acc: [0.9727093252816166, 0.9742190221809314, 0.9725931947509, 0.9739867611194983, 0.9736383695273487]\n",
      "F1: [0.9707955689828802, 0.9721115537848606, 0.9695729992329327, 0.9719743621968078, 0.9710052369395835]\n",
      "train trial metrics: [1.0, 1.0, 1.0]\n",
      "test trial metrics: [0.996 0.973 0.971]\n",
      "trial: 0\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "Raw test values\n",
      "AUC: [0.9906156045037205, 0.9914715482242094, 0.9910284925030058, 0.9905158181316122, 0.9907986571741171]\n",
      "Acc: [0.9460666666666666, 0.9483333333333334, 0.9463333333333334, 0.9458, 0.9492]\n",
      "F1: [0.9459694116075603, 0.9482874412357287, 0.947354302193012, 0.9455153949129853, 0.9481182795698925]\n",
      "train trial metrics: [1.0, 1.0, 1.0]\n",
      "test trial metrics: [0.991 0.947 0.947]\n",
      "trial: 0\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "Raw test values\n",
      "AUC: [0.9020488873165246, 0.9020604394859583, 0.8986109577799153, 0.9010217851335058, 0.8993677788329375]\n",
      "Acc: [0.8232554182898968, 0.8195714672610988, 0.8185801684687125, 0.8200992340437352, 0.8212693485552385]\n",
      "F1: [0.8226898891495652, 0.8190312510960681, 0.8178036213183585, 0.816701029466987, 0.8185276971786889]\n",
      "train trial metrics: [1.0, 1.0, 1.0]\n",
      "test trial metrics: [0.901 0.821 0.819]\n",
      "trial: 0\n",
      "trial: 1\n",
      "trial: 2\n",
      "trial: 3\n",
      "trial: 4\n",
      "Raw test values\n",
      "AUC: [0.9584702330896755, 0.9589003064393685, 0.9561498555580314, 0.9576883362044625, 0.9612636894168683]\n",
      "Acc: [0.9015984654731458, 0.9040281329923273, 0.8998721227621483, 0.8966112531969309, 0.9052429667519182]\n",
      "F1: [0.9316575722336524, 0.9326083080763059, 0.9305074202434905, 0.9284089388465141, 0.9339364894522327]\n",
      "train trial metrics: [1.0, 1.0, 1.0]\n",
      "test trial metrics: [0.958 0.901 0.931]\n",
      "Random Forests metrics train:\n",
      " [[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "Average Random Forests train metrics:\n",
      " [1. 1. 1.]\n",
      "Random Forests metrics test:\n",
      " [[0.9955677  0.97342933 0.97109194]\n",
      " [0.99088602 0.94714667 0.94704897]\n",
      " [0.90062197 0.82055513 0.8189507 ]\n",
      " [0.95849448 0.90147059 0.93142375]]\n",
      "Average Random Forests test metrics:\n",
      " [0.96139255 0.91065043 0.91712884]\n",
      "Average across Random Forests test metrics:\n",
      " [0.98002966 0.96169389 0.84670926 0.93046294]\n"
     ]
    }
   ],
   "source": [
    "# Random Forests model \n",
    "# store metrics of all 4 datasets \n",
    "rf_metrics_train = []\n",
    "rf_metrics_test = []\n",
    "for X, y in zip(X_total, y_total):\n",
    "    # create 3 lists for the AUC, Accuracy, and F1 Scores across the 5 trials\n",
    "    rf_AUC_train, rf_Acc_train, rf_F1_train = [], [], []\n",
    "    rf_AUC_test, rf_Acc_test, rf_F1_test = [], [], []\n",
    "    for trial in range(5):\n",
    "        print(\"trial:\", trial)\n",
    "        # for each trial, randomly select 5000 samples for the training set & rest as test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000) \n",
    "        max_features = [1, 2, 4, 6, 8, 12, 16, 20]\n",
    "        # metrics to evaluate model on\n",
    "        scoring = {\"Accuracy\": make_scorer(accuracy_score), \"F1_Score\": \"f1\", \"AUC\": \"roc_auc\"}\n",
    "\n",
    "        # Create a pipeline\n",
    "        pipe = Pipeline([('std', StandardScaler()), \n",
    "                         ('classifier', RandomForestClassifier(n_estimators=1024))])\n",
    "        # Create search space of candidate learning algorithms and their hyperparameters\n",
    "        search_space = [{'classifier': [RandomForestClassifier(n_estimators=1024)], \n",
    "                         'classifier__max_features': max_features}]\n",
    "        # Grid Search with stratified 5 folds cross validation to find best hyperparmeters\n",
    "        search_results = GridSearchCV(pipe, search_space, scoring = scoring, \n",
    "                                      refit = False, cv=StratifiedKFold(n_splits = 5))\n",
    "        # train models\n",
    "        search_results.fit(X_train, y_train)\n",
    "\n",
    "        # find optimal hyperparameters for each metric\n",
    "        AUC_rank = search_results.cv_results_[\"rank_test_AUC\"]\n",
    "        # find index of #1 in the rank array to find best hyperparameter\n",
    "        AUC_ind = np.argmin(AUC_rank)\n",
    "        # save best hyperparameters\n",
    "        opt_AUC = search_results.cv_results_[\"params\"][AUC_ind]\n",
    "\n",
    "        Acc_rank = search_results.cv_results_[\"rank_test_Accuracy\"]\n",
    "        # find index of #1 in the rank array to find best hyperparameter\n",
    "        Acc_ind = np.argmin(Acc_rank) \n",
    "        # save best hyperparameters\n",
    "        opt_Acc = search_results.cv_results_[\"params\"][Acc_ind]\n",
    "\n",
    "        F1_rank = search_results.cv_results_[\"rank_test_F1_Score\"]\n",
    "        # find index of #1 in the rank array to find best hyperparameter\n",
    "        F1_ind = np.argmin(F1_rank)\n",
    "        # save best hyperparameters\n",
    "        opt_F1 = search_results.cv_results_[\"params\"][F1_ind]\n",
    "\n",
    "        # train 3 models with the optimal parameters -- one model for each metric\n",
    "        # AUC model\n",
    "        rf_AUC_model = RandomForestClassifier(n_estimators=1024, \n",
    "                                              max_features = opt_AUC[\"classifier__max_features\"]) \n",
    "        rf_AUC_model.fit(X_train, y_train)\n",
    "        # make predictions & calculate AUC on both training and testing sets\n",
    "        rf_AUC_pred_train = (rf_AUC_model.predict_proba(X_train)[:,1])\n",
    "        rf_AUC_score_train = roc_auc_score(y_train, rf_AUC_pred_train)\n",
    "        rf_AUC_pred_test = (rf_AUC_model.predict_proba(X_test)[:,1])\n",
    "        rf_AUC_score_test = roc_auc_score(y_test, rf_AUC_pred_test)\n",
    "        # add AUC for current trial \n",
    "        rf_AUC_train.append(rf_AUC_score_train)\n",
    "        rf_AUC_test.append(rf_AUC_score_test)\n",
    "        \n",
    "        # Accuracy model\n",
    "        rf_Acc_model = RandomForestClassifier(n_estimators=1024, \n",
    "                                              max_features = opt_Acc[\"classifier__max_features\"])\n",
    "        rf_Acc_model.fit(X_train, y_train)     \n",
    "        # calculate accuracy on both training and testing sets \n",
    "        rf_Acc_score_train = rf_Acc_model.score(X_train, y_train)\n",
    "        rf_Acc_score_test = rf_Acc_model.score(X_test, y_test)\n",
    "        # add Accuracy for current trial \n",
    "        rf_Acc_train.append(rf_Acc_score_train)\n",
    "        rf_Acc_test.append(rf_Acc_score_test)\n",
    "\n",
    "        # F1 Score model\n",
    "        rf_F1_model = RandomForestClassifier(n_estimators=1024, \n",
    "                                             max_features = opt_F1[\"classifier__max_features\"])\n",
    "        rf_F1_model.fit(X_train, y_train)\n",
    "        # make predictions & calculate F1 score on both training and testing sets \n",
    "        rf_F1_pred_train = (rf_F1_model.predict(X_train))\n",
    "        rf_F1_score_train = f1_score(y_train, rf_F1_pred_train)\n",
    "        rf_F1_pred_test = (rf_F1_model.predict(X_test))\n",
    "        rf_F1_score_test = f1_score(y_test, rf_F1_pred_test)\n",
    "        # add F1 score for current trial \n",
    "        rf_F1_train.append(rf_F1_score_train)\n",
    "        rf_F1_test.append(rf_F1_score_test)\n",
    "\n",
    "    # average AUC, Accuracy, and F1 score across all 5 trials \n",
    "    rf_AUC_train_m, rf_AUC_test_m = np.mean(rf_AUC_train), np.mean(rf_AUC_test)\n",
    "    rf_Acc_train_m, rf_Acc_test_m = np.mean(rf_Acc_train), np.mean(rf_Acc_test)\n",
    "    rf_F1_train_m, rf_F1_test_m = np.mean(rf_F1_train), np.mean(rf_F1_test)     \n",
    "    # combine average training metrics into one array for current trial\n",
    "    rf_trial_metrics_train = [rf_AUC_train_m, rf_Acc_train_m, rf_F1_train_m]\n",
    "    # combine average testing metrics into one array for current trial\n",
    "    rf_trial_metrics_test = [rf_AUC_test_m, rf_Acc_test_m, rf_F1_test_m]\n",
    "    # add average train and test metrics for current trial\n",
    "    rf_metrics_train.append(rf_trial_metrics_train)\n",
    "    rf_metrics_test.append(rf_trial_metrics_test)\n",
    "    \n",
    "    # print raw test metric values for current trial\n",
    "    print(\"Raw test values\")\n",
    "    print(\"AUC:\", rf_AUC_test)\n",
    "    print(\"Acc:\", rf_Acc_test)\n",
    "    print(\"F1:\", rf_F1_test)\n",
    "    # print average metrics for train and test for current trial\n",
    "    print(\"train trial metrics:\", rf_trial_metrics_train)\n",
    "    print(\"test trial metrics:\", np.round(rf_trial_metrics_test, 3))\n",
    "    \n",
    "# final metrics from all 4 datasets\n",
    "rf_metrics_train = np.array(rf_metrics_train)\n",
    "print(\"Random Forests metrics train:\\n\", rf_metrics_train)\n",
    "rf_metrics_train_m = np.mean(rf_metrics_train, axis=0)\n",
    "print(\"Average Random Forests train metrics:\\n\", rf_metrics_train_m)  \n",
    "rf_metrics_test = np.array(rf_metrics_test)\n",
    "print(\"Random Forests metrics test:\\n\", rf_metrics_test)\n",
    "rf_metrics_test_m = np.mean(rf_metrics_test, axis=0)\n",
    "print(\"Average Random Forests test metrics:\\n\", rf_metrics_test_m)\n",
    "# calculate average across metrics \n",
    "rf_metrics_test_m_2 = np.mean(rf_metrics_test, axis=1)\n",
    "print(\"Average across Random Forests test metrics:\\n\", rf_metrics_test_m_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the full arrays for the rf values in both tables 2 and 3, and will be used to calculate the t tests and p values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 2 rf arrays (cols of rf metrics test)\n",
    "rf_auc = [0.9955677, 0.99088602, 0.90062197, 0.95849448]\n",
    "rf_acc = [0.97342933, 0.94714667, 0.82055513, 0.90147059]\n",
    "rf_f1 = [0.97109194, 0.94704897, 0.8189507, 0.93142375]\n",
    "rf_mean_1 = [0.96139255, 0.91065043, 0.91712884]\n",
    "\n",
    "# table 3 rf arrays (averaged AUC, ACC, and F1 metrics for all trials per dataset)\n",
    "rf_d1 = [0.9797045134020594, 0.9806274803022443, 0.9791574233570102, \n",
    "         0.9804438498292019, 0.9802150355895057]\n",
    "rf_d2 = [0.9608838942593159, 0.9626974409310906, 0.9615720426764504, \n",
    "         0.9606104043481992, 0.9627056455813365]\n",
    "rf_d3 = [0.8493313982519956, 0.8468877192810417, 0.8449982491889955, \n",
    "         0.8459406828814093, 0.8463882748556216]\n",
    "rf_d4 = [0.9305754235988246, 0.9318455825026671, 0.9288431328545568, \n",
    "         0.9275695094159692, 0.9334810485403398]\n",
    "rf_mean_2 = [0.98002966, 0.96169389, 0.84670926, 0.93046294]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: [0.86647845 0.79874321 0.81167886]\n",
      "svm: [0.84012632 0.76755563 0.79634207]\n",
      "knn: [0.85787983 0.81548121 0.84312728]\n",
      "rf: [0.96139255 0.91065043 0.91712884]\n",
      "Average Metrics across Datasets per Model:\n",
      " [[0.86647845 0.79874321 0.81167886]\n",
      " [0.84012632 0.76755563 0.79634207]\n",
      " [0.85787983 0.81548121 0.84312728]\n",
      " [0.96139255 0.91065043 0.91712884]]\n"
     ]
    }
   ],
   "source": [
    "# table 2\n",
    "print(\"lr:\", lr_metrics_test_m)\n",
    "print(\"svm:\", svm_metrics_test_m)\n",
    "print(\"knn:\", knn_metrics_test_m)\n",
    "print(\"rf:\", rf_metrics_test_m)\n",
    "\n",
    "final_metrics = np.array([lr_metrics_test_m, svm_metrics_test_m, \n",
    "                          knn_metrics_test_m, rf_metrics_test_m])\n",
    "# models in rows and metrics in columns\n",
    "print(\"Average Metrics across Datasets per Model:\\n\", final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean values: [0.82563351 0.80134134 0.83882944 0.92972394]\n"
     ]
    }
   ],
   "source": [
    "# calculate mean values across metrics from table 2\n",
    "t2_mean_metrics = np.mean(final_metrics, axis = 1)\n",
    "\n",
    "print(\"mean values:\", t1_mean_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: [0.93024686 0.75435848 0.74932215 0.86860654]\n",
      "svm: [0.81255548 0.90328871 0.7105528  0.77896837]\n",
      "knn: [0.85908809 0.96847997 0.80633238 0.72141731]\n",
      "rf: [0.98002966 0.96169389 0.84670926 0.93046294]\n",
      "Average Metrics for all Algorithms:\n",
      " [[0.93024686 0.75435848 0.74932215 0.86860654]\n",
      " [0.81255548 0.90328871 0.7105528  0.77896837]\n",
      " [0.85908809 0.96847997 0.80633238 0.72141731]\n",
      " [0.98002966 0.96169389 0.84670926 0.93046294]]\n"
     ]
    }
   ],
   "source": [
    "# table 3\n",
    "# calculate average metrics for each algorithm across metrics \n",
    "print(\"lr:\", lr_metrics_test_m_2)\n",
    "print(\"svm:\", svm_metrics_test_m_2)\n",
    "print(\"knn:\", knn_metrics_test_m_2)\n",
    "print(\"rf:\", rf_metrics_test_m_2)\n",
    "\n",
    "final_metrics_2 = np.array([lr_metrics_test_m_2, svm_metrics_test_m_2, \n",
    "                            knn_metrics_test_m_2, rf_metrics_test_m_2])\n",
    "# models in rows and datasets in columns\n",
    "print(\"Average Metrics for all Algorithms:\\n\", final_metrics_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean values: [0.82563351 0.80134134 0.83882944 0.92972394]\n"
     ]
    }
   ],
   "source": [
    "# calculate mean values across datasets from table 1\n",
    "t3_mean_datasets = np.mean(final_metrics_2, axis = 1)\n",
    "\n",
    "print(\"mean values:\", t2_mean_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating T-tests and P values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 2 lr arrays (cols of Lr metrics test)\n",
    "lr_auc = [0.96856484, 0.81103193, 0.79646982, 0.88984722]\n",
    "lr_acc = [0.91464406, 0.72506667, 0.72423908, 0.83102302]\n",
    "lr_f1 = [0.90753167, 0.72697683, 0.72725756, 0.88494939]\n",
    "lr_mean_1 = [0.86647845, 0.79874321, 0.81167886]\n",
    "\n",
    "# table 3 lr arrays (averaged AUC, ACC, and F1 metrics for all trials per dataset)\n",
    "lr_d1 = [0.9586460235320803, 0.9565694684321265, 0.9608197341226062, \n",
    "         0.8157598277168324, 0.959439234438331]\n",
    "lr_d2 = [0.7565088702051469, 0.7535006000863818, 0.7573735514758195, \n",
    "         0.7522884514861006, 0.752120912899643]\n",
    "lr_d3 = [0.751667668556535, 0.7509692372282948, 0.74418526232208, \n",
    "         0.7448603087168252, 0.7549282796006591]\n",
    "lr_d4 = [0.8688679110624338, 0.8708719759122815, 0.8668214982190855, \n",
    "         0.8685907654597697, 0.8678805680443218]\n",
    "lr_mean_2 = [0.93024686, 0.75435848, 0.74932215, 0.86860654]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 2 svm arrays (cols of svm metrics test)\n",
    "svm_auc = [0.86308537, 0.9519339, 0.75460011, 0.79088587]\n",
    "svm_acc = [0.78283591, 0.87936, 0.694689, 0.7133376]\n",
    "svm_f1 = [0.79174517, 0.87857221, 0.68236928, 0.83268163]\n",
    "svm_mean_1 = [0.84012632, 0.76755563, 0.79634207]\n",
    "\n",
    "# table 3 svm arrays (averaged AUC, ACC, and F1 metrics for all trials per dataset)\n",
    "svm_d1 = [0.8117481691067928, 0.8114812624881037, 0.8143406000018384, \n",
    "          0.8101530147623502, 0.8150543707217408]\n",
    "svm_d2 = [0.9035421730714287, 0.9037623389854074, 0.9011522524561654, \n",
    "          0.9041148849546022, 0.9038718810307453]\n",
    "svm_d3 = [0.7089026670765787, 0.7165935378258358, 0.7064106814361778, \n",
    "          0.7099869710363361, 0.7108701247511338]\n",
    "svm_d4 = [0.7896899125598481, 0.7653704630352699, 0.7833403412046526, \n",
    "          0.7772230946078841, 0.7792180183022389]\n",
    "svm_mean_2 = [0.81255548, 0.90328871, 0.7105528, 0.77896837]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 2 knn arrays (cols of knn metrics test)\n",
    "knn_auc = [0.91618275, 0.99060503, 0.86008995, 0.66464157]\n",
    "knn_acc = [0.83272558, 0.95746667, 0.7805945, 0.69113811]\n",
    "knn_f1 = [0.82835594, 0.95736822, 0.77831268, 0.80847226]\n",
    "knn_mean_1 = [0.85787983, 0.81548121, 0.84312728]\n",
    "\n",
    "# table 3 knn arrays (averaged AUC, ACC, and F1 metrics for all trials per dataset)\n",
    "knn_d1 = [0.8650658088942013, 0.8363658225559866, 0.8634172401507417, \n",
    "          0.8611621086321147, 0.8694294834315387]\n",
    "knn_d2 = [0.9697651341969605, 0.9682222191846863, 0.9683229017355536, \n",
    "          0.9685493101230817, 0.9675402988951393]\n",
    "knn_d3 = [0.8048035814698253, 0.8051453861700839, 0.8065270214822863, \n",
    "          0.8050974487241892, 0.8100884609532483]\n",
    "knn_d4 = [0.7264361218729348, 0.7199983760968302, 0.7198371276286712, \n",
    "          0.7204015251781053, 0.7204134129556534]\n",
    "knn_mean_2 = [0.85908809, 0.96847997, 0.80633238, 0.72141731]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 2 rf arrays (cols of rf metrics test)\n",
    "rf_auc = [0.9955677, 0.99088602, 0.90062197, 0.95849448]\n",
    "rf_acc = [0.97342933, 0.94714667, 0.82055513, 0.90147059]\n",
    "rf_f1 = [0.97109194, 0.94704897, 0.8189507, 0.93142375]\n",
    "rf_mean_1 = [0.96139255, 0.91065043, 0.91712884]\n",
    "\n",
    "# table 3 rf arrays (averaged AUC, ACC, and F1 metrics for all trials per dataset)\n",
    "rf_d1 = [0.9797045134020594, 0.9806274803022443, 0.9791574233570102, \n",
    "         0.9804438498292019, 0.9802150355895057]\n",
    "rf_d2 = [0.9608838942593159, 0.9626974409310906, 0.9615720426764504, \n",
    "         0.9606104043481992, 0.9627056455813365]\n",
    "rf_d3 = [0.8493313982519956, 0.8468877192810417, 0.8449982491889955, \n",
    "         0.8459406828814093, 0.8463882748556216]\n",
    "rf_d4 = [0.9305754235988246, 0.9318455825026671, 0.9288431328545568, \n",
    "         0.9275695094159692, 0.9334810485403398]\n",
    "rf_mean_2 = [0.98002966, 0.96169389, 0.84670926, 0.93046294]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of table 2's values, it is evident that the Random Forests model performed the best in all three metrics, so those will be the values compared to in the Independent t-test. For example, the p value between rf_auc and lr_auc calculation is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09458285528558459"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Independent t-test \n",
    "stat, p_val = stats.ttest_ind(np.array(rf_auc), np.array(lr_auc), equal_var = False)\n",
    "p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of table 3's values, it is evident that the Random Forests model performed the best in datasets 1, 3, and 4, with the exception of dataset 2 where KNN slightly outperformed. So those will be the values compared to in the Independent t-test. For example, the p value between rf_d1 and lr_d1 calculation is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15705374922671014"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Independent t-test \n",
    "stat, p_val = stats.ttest_ind(np.array(rf_d1), np.array(lr_d1), equal_var = False)\n",
    "p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr train mean: [0.87092374 0.8027     0.81679145]\n",
      "svm train mean: [0.85132273 0.77348    0.80148168]\n",
      "knn train mean: [1.        0.96671   0.9700947]\n",
      "rf train mean: [1. 1. 1.]\n",
      "Average Training set Metrics across Datasets per Model:\n",
      " [[0.87092374 0.8027     0.81679145]\n",
      " [0.85132273 0.77348    0.80148168]\n",
      " [1.         0.96671    0.9700947 ]\n",
      " [1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# secondary table 1 -- mean training set performance \n",
    "print(\"lr train mean:\", lr_metrics_train_m)\n",
    "print(\"svm train mean:\", svm_metrics_train_m)\n",
    "print(\"knn train mean:\", knn_metrics_train_m)\n",
    "print(\"rf train mean:\", rf_metrics_train_m)\n",
    "\n",
    "final_metrics_train = np.array([lr_metrics_train_m, svm_metrics_train_m, \n",
    "                                knn_metrics_train_m, rf_metrics_train_m])\n",
    "# models in rows and metrics in columns\n",
    "print(\"Average Training set Metrics across Datasets per Model:\\n\", final_metrics_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean values: [0.8301384  0.80876147 0.9789349  1.        ]\n"
     ]
    }
   ],
   "source": [
    "# calculate mean values across metrics from secondary table 1\n",
    "st1_mean_datasets = np.mean(final_metrics_train, axis = 1)\n",
    "\n",
    "print(\"mean values:\", st1_mean_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student's I discussed with:\n",
    "- Anjali Ramesh\n",
    "- Urmi Suresh\n",
    "- Harmeena Sandhu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
